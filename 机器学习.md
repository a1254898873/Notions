# 机器学习

## 流程

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151952.png)



## 经验风险和风险函数

### 一、概念

经验风险( Empirical Risk) : 损失函数度量了单个样本的预测结果，要想衡量整个训练集的预测值与真实值的差异,将整个训练集所有记录均进行一次预测, 求取损失函数,将所有值累加,即为经验风险。

就是已知的数据按照现有的模型，测试预测值和真实值偏离的程度叫经验风险。

经验风险越小说明模型f(x)对训练集的拟合程度越好。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151953.png)



 风险函数( Risk Function) :又称期望损失、期望风险。所有数据集(包括训练集和预测集,遵循联合分布P(X,Y) )的损失函数的期望值。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151954.png)



### 二、差异

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151955.png)





### 三、经验风险的问题

在样本较小时，仅关注经验风险，很容易就导致过拟合。

过拟合：对当前的样本数据效果非常好，损失函数什么的非常低，但是如果碰到新的数据集，预测效果很差。





### 四、结构风险

结构风险( Structural Risk) : 在经验风险的基础上，增加一个正则化项( Regularizer )或者叫做惩罚项( Penalty Term) , 公式为:

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151956.png)

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151957.png)







## 机器学习中的概率

### 先验概率

百度百科定义：先验概率（prior probability）是指根据以往经验和分析得到的概率，如全概率公式，它往往作为"由因求果"问题中的"因"出现的概率。

维基百科定义: 在贝叶斯统计中，某一不确定量p的先验概率分布是在考虑"观测数据"前，能表达p不确定性的概率分布。

先验概率是基于背景常识或者历史数据的统计得出的预判概率，一般只包含一个变量，例如p(x)，p(y)。

可以看到二者定义有一个共同点，即先验概率是不依靠观测数据的概率分布，也就是与其他因素独立的分布。所以可以用P(θ)表示。



### 后验概率

维基百科定义: 在贝叶斯统计中，一个随机事件或者一个不确定事件的后验概率是在考虑和给出相关证据或数据后所得到的条件概率。同样，后验概率分布是一个未知量（视为随机变量）基于试验和调查后得到的概率分布。

简单的理解就是这个概率需要机遇观测数据才能得到，例如我们需要对一个神经网络建模，我们需要基于给定的数据集X才能得到网络参数θ的分布，所以后验概率表示为P(θ|X)



### 似然概率

百度百科定义: 统计学中，似然函数是一种关于统计模型参数的函数。给定输出x时，关于参数θ的似然函数L(θ|x)（在数值上）等于给定参数θ后变量X的概率：L(θ|x)=P(X=x|θ)。

维基百科定义: 在数理统计学中，似然函数是一种关于统计模型中的参数的函数，表示模型参数中的似然性。



### 关系

![概率与似然](https://raw.githubusercontent.com/a1254898873/images/master/202203241151958.png)

c为参数





## 最大似然估计

似然反映的是：已知结果，反推原因。具体而言，似然（Likelihood）函数表示的是基于观察的数据，取不同的参数θ时，统计模型以多大的可能性接近真实观察数据。

似然函数是关于统计模型参数的函数，是描述观察到的真实数据在不同参数下发生的概率。最大似然估计要寻找最优参数，让似然函数最大化。或者说，使用最优参数时观测数据发生的概率最大。

最小二乘与最大似然的公式几乎一样。直观上来说，最小二乘法是在寻找观测数据与回归超平面之间的误差距离最小的参数。最大似然估计是最大化观测数据发生的概率。当我们假设误差是正态分布的，所有误差项越接近均值0，概率越大。正态分布是在均值两侧对称的，误差项接近均值的过程等同于距离最小化的过程。







## 偏差与方差



<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151959.png" alt="img" style="zoom:50%;" />

偏差（Bias）：是指预测值和真实值之间的差，偏差越大，预测和真实值之间的差别越大，衡量模型的预测能力。

方差（Variance）：描述预测值的变化范围和离散程度，方差越大，表示预测值的分布越零散，对象是多个模型，使用不同的训练数据训练出的模型差别有多大。

当训练误差和交叉验证误差或测试误差都很大，且值差不多时，处于高偏差，低方差，欠拟合状态；当训练误差和交叉验证误差差别很大，且测试集误差小，验证集误差大时，处于高方差，低偏差，过拟合状态。







## 机器学习性能评价指标



![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151960.png)



1、精确率（Precision）

即正确预测为正的占全部预测为正的比例。



2、召回率（Recall）

即正确预测为正的占全部实际为正的比例。



3、准确率（Accuracy）

就是所有的预测正确（正类负类）的占总的比重。



4、F1值

它是精确率和召回率的调和平均数，最大为1，最小为0。



5、ROC曲线、AUC值

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151961.png)

横坐标：1-Specificity，伪正类率(False positive rate，FPR，FPR=FP/(FP+TN))，预测为正但实际为负的样本占所有负例样本的比例；

纵坐标：Sensitivity，真正类率(True positive rate，TPR，TPR=TP/(TP+FN))，预测为正且实际为正的样本占所有正例样本的比例。

真正的理想情况，TPR应接近1，FPR接近0，即图中的（0,1）点。ROC曲线越靠拢（0,1）点，越偏离45度对角线越好。

AUC (Area Under Curve) 被定义为ROC曲线下的面积，显然这个面积的数值不会大于1。又由于ROC曲线一般都处于y=x这条直线的上方，所以AUC的取值范围一般在0.5和1之间。使用AUC值作为评价标准是因为很多时候ROC曲线并不能清晰的说明哪个分类器的效果更好，而作为一个数值，对应AUC更大的分类器效果更好。


从AUC判断分类器（预测模型）优劣的标准：

AUC = 1，是完美分类器，采用这个预测模型时，存在至少一个阈值能得出完美预测。绝大多数预测的场合，不存在完美分类器。
0.5 < AUC < 1，优于随机猜测。这个分类器（模型）妥善设定阈值的话，能有预测价值。
AUC = 0.5，跟随机猜测一样（例：丢铜板），模型没有预测价值。
AUC < 0.5，比随机猜测还差；但只要总是反预测而行，就优于随机猜测。













## pandas

### 1、填充缺失值

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151962.png)

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151963.png)





### 2、丢弃缺失值

```python
data.dropna(how = 'all')    # 传入这个参数后将只丢弃全为缺失值的那些行
data.dropna(axis = 1)       # 丢弃有缺失值的列（一般不会这么做，这样会删掉一个特征）
data.dropna(axis=1,how="all")   # 丢弃全为缺失值的那些列
data.dropna(axis=0,subset = ["Age", "Sex"])   # 丢弃‘Age’和‘Sex’这两列中有缺失值的行    
```



### 3、转化为onthot编码

get_dummies 是利用pandas实现one hot encode的方式。

```python
df = pd.get_dummies(df)
```

```python
pd.get_dummies(df.color)
```



### 4、转化为张量

```python
x = torch.tensor(inputs.values)
```



### 5、删除重复行

```python
df.drop_duplicates(inplace=True)
```



### 6、异常值处理

```python
import  pandas as pd


# 正态分布
# 3sigma --->  mean() - 3* std() ---下限
#  mean() + 3* std() ---上限

# 自实现3sigma 原则

def three_sigma(ser):
    """
    自实现3sigma 原则
    :param ser: 数据
    :return: 处理完成的数据
    """
    bool_id = ((ser.mean() - 3 * ser.std()) <= ser)  &   (ser <= (ser.mean() + 3 * ser.std()))

    # bool数组索引

    # ser[bool_id]

    return ser.index[bool_id]

#使用detail 验证
deatil = pd.read_excel("./meal_order_detail.xlsx")

print(deatil.shape)

# 调用3sigma原则，进行异常值过滤
index_name_list = three_sigma(deatil['amounts'])

deatil = deatil.loc[index_name_list,:]

print(deatil.shape)

#percentile() 计算分位数

# np.percentile() ql-1.5iqr  qu + 1.5iqr
```





## sklearn

### 1、获取数据

```python
from sklearn import datasets
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151965.awebp)

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151966.awebp)



```python
1. sklearn.datasets
    1.1 加载获取流行数据集
    1.2 datasets.load_*() -- 获取小规模数据集，数据包含在datasets里
    1.3 datasetss.fetch_*(data_home=None)
    获取大规模数据集，需要从网络上下载，函数的第一个参数是data_home， 表示数据集。下载目录,默认是-/scikit-learn_data/
2. load_* 和 fetch_* 返回的数据类型是datasets.base.Bunch(字典格式)
    data:特征数据数组,是[n_ samples*n_features]的二维numpy.ndarray数组
    target:标签数组,是n_samples的维numpy.ndarray数组
    DESCR:数据描述
    feature_names:特征名，新闻数据，手写数字，回归数据集没有
    target_names;标签名
#关于第二点, load_* 用于获取小数据集 , fetch_* 用于获取大数据集

iris = datasets.load_iris() # 导入数据集
Iris = load_iris()
print("查看数据描述:",Iris["DESCR"])
print("查看特征值名字：",Iris.feature_name)
print("查看特征值：",Iris.data)
print("查看数据集： ",Iris)

```



### 2、数据集划分

数据集会划分成两部分：训练集和测试集。
训练集用来训练，构建模型，测试集用来评估预测结果。
比例一般为75%的数据为训练集，25%的数据为测试集。

```python
from sklearn.model_selection import train_test_split
# x_train ,x_test ,y_train , y_test = train_test_split(x,y,test_size=0.*) 
# x : 数据集的特征值
# y : 数据集的目标值
# test_size : 测试集的大小,一般为float
# radom_state : 随机数种子,不同的种子会造成不同的随机采样结果.相同的种子采样结果相同
# return -- 方法返回4个结果: 训练集特征值, 测试集特征值, 训练集目标值, 测试值目标值(默认随机取值)
```



### 3、数据预处理

#### 3.1 缺失值处理

 （1）直接丢掉带有缺失值的行/列

```python
reduced_X_train = X_train.dropna(axis = 1)
reduced_X_valid = X_valid.dropna(axis = 1)
```

axis = 1是丢掉列，axis = 0是丢掉行。



（2）Imputation

Imputation就是用每一列的均值/中位数/最大频率的数等去补充缺失值。值得注意的是对于valid的数据而言，fit的时候仍然要用train的数据。strategy也可以修改为其他的方法。

```python
sklearn缺失值接口 : sklearn.impute.SimpleImputer
   SimpleImputer(missing_values='NaN',strategy='mean',axis=0) 
       完成缺失值插补
   SimpleImputer.fit_transform(x) 
        x:numpy array格式的数据 
        返回值：转换后形状相同也即是的array 
步骤
    初始化SimpleImputer,指定缺失值，指定填补策略，指定行或列。 注：缺失值也可以是别的指定要替换的值 
    调用　ﬁt_transform
```

```python
import numpy as np
from sklearn.impute import SimpleImputer
data = np.array([[1,2],[np.NaN,3],[7,6]])
si = SimpleImputer()
# si = SimpleImputer(missing_values=6,strategy='most_frequent')  
# missing_values : 指定值填充,默认为NaN. 
# strategy : 填充数值计算方法  ['mean', 'median', 'most_frequent', 'constant']
res = si.fit_transform(data)
print(res)
```

```python
from numpy import vstack, array, nan
from sklearn.preprocessing import Imputer

#缺失值计算，返回值为计算缺失值后的数据
#参数missing_value为缺失值的表示形式，默认为NaN
#参数strategy为缺失值填充方式，默认为mean（均值）
Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))
```



#### 3.2 one-hot编码

对类别特征进行One-hot编码

```python
# 导入接口
from sklearn.feature_extraction import DictVectorizer
# 数据
data = [{'name':'汉唐店','Satisfaction':3.5},
        {'name':'花溪店','Satisfaction':2.5},
        {'name':'温泉店','Satisfaction':3}] 
# 实例化
dv = DictVectorizer()
# 调用转换接口
res = dv.fit_transform(data)
print(res.toarray())   # 转换成数组  看起来更清晰
# 结果注解👇 : 第一个特征值 是3.5 所以第1个 是1  ,二个是2.5≠3.5  所以是0  ,第三个值是3 ≠3.5  ,所以是0 ；第二,第三列类似第一种
dv.get_feature_names()   # 特征名称
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151967.png)

```python
 from sklearn.preprocessing import OneHotEncoder
 
 #哑编码，对IRIS数据集的目标值，返回值为哑编码后的数据
 OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))
```





#### 3.3 标准化

![[公式]](https://www.zhihu.com/equation?tex=%5Cfrac%7Bx_i-x_%7Bmean%7D%7D%7Bx_%7Bstd%7D%7D)

将数据变换为均值为0，标准差为1的分布切记，并非一定是正态的

- 缩放到均值为0，方差为1（Standardization——StandardScaler()）
- 缩放到0和1之间，保留原始数据的分布（Normalization——Normalizer()）

由于归一化很容易受到超大数据的干扰，稳定性较差（在数据相差较大的场景当中）所以我们这里还有一个叫做标准化。

```python
om sklearn import preprocessing
```

```python
data = [[0, 0], [0, 0], [1, 1], [1, 1]]
# 1. 基于mean和std的标准化
scaler = preprocessing.StandardScaler().fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
```

```python
 from sklearn.preprocessing import StandardScaler
 
 #标准化，返回值为标准化后的数据
 StandardScaler().fit_transform(iris.data)

```



#### 3.4 归一化

![image-20220320170338528](https://raw.githubusercontent.com/a1254898873/images/master/202203241151968.png)

将一列数据变化到某个固定区间(范围)中，通常，这个区间是[0, 1]

```python

# 2. 将每个特征值归一化到一个固定范围
scaler = preprocessing.MinMaxScaler(feature_range=(0, 1)).fit(train_data)
scaler.transform(train_data)
scaler.transform(test_data)
#feature_range: 定义归一化范围，注用（）括起来
```

现实中，用归一化更多， 因为需要严格要求数据在0到1之间且量纲相等。

标准化没有改变原始数据的极值例子，而归一化则将所有数据都压缩到了01区间内，改变了数据的分布。



#### 3.5 正则化

```python
sklearn.preprocessing.Normalizer(norm=’l2’, copy=True)
```

对样本进行l1或者l2正则化，使其l1或l2范数为1

该方法主要应用于文本分类和聚类中。

l1正则化的计算方式：

![image-20220316172043606](https://raw.githubusercontent.com/a1254898873/images/master/202203241151969.png)

l2正则化的计算方式：

![image-20220316172134183](https://raw.githubusercontent.com/a1254898873/images/master/202203241151970.png)

作用：可以减少过拟合

```python
normalizer = preprocessing.Normalizer().fit(x)
normalizer.transform(x)
```



#### 3.6 二值化

定量特征二值化的核心在于设定一个阈值，大于阈值的赋值为1，小于等于阈值的赋值为0，公式表达如下：

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151971.png)

```python
 from sklearn.preprocessing import Binarizer
 
 #二值化，阈值设置为3，返回值为二值化后的数据
 Binarizer(threshold=3).fit_transform(iris.data)
```



### 4、特征选择

　　当数据预处理完成后，我们需要选择有意义的特征输入机器学习的算法和模型进行训练。通常来说，从两个方面考虑来选择特征：

- 特征是否发散：如果一个特征不发散，例如方差接近于0，也就是说样本在这个特征上基本上没有差异，这个特征对于样本的区分并没有什么用。
- 特征与目标的相关性：这点比较显见，与目标相关性高的特征，应当优选选择。除方差法外，本文介绍的其他方法均从相关性考虑。

　　根据特征选择的形式又可以将特征选择方法分为3种：

- Filter：过滤法，按照发散性或者相关性对各个特征进行评分，设定阈值或者待选择阈值的个数，选择特征。
- Wrapper：包装法，根据目标函数（通常是预测效果评分），每次选择若干特征，或者排除若干特征。
- Embedded：嵌入法，先使用某些机器学习的算法和模型进行训练，得到各个特征的权值系数，根据系数从大到小选择特征。类似于Filter方法，但是是通过训练来确定特征的优劣。

　　我们使用sklearn中的feature_selection库来进行特征选择。



#### 4.1 filter

4.1.1  方差选择法

使用方差选择法，先要计算各个特征的方差，然后根据阈值，选择方差大于阈值的特征。使用feature_selection库的VarianceThreshold类来选择特征的代码如下：

```python
 from sklearn.feature_selection import VarianceThreshold
 
 #方差选择法，返回值为特征选择后的数据
 #参数threshold为方差的阈值
 VarianceThreshold(threshold=3).fit_transform(iris.data)
```



4.1.2  相关系数法

使用相关系数法，先要计算各个特征对目标值的相关系数以及相关系数的P值。用feature_selection库的SelectKBest类结合相关系数来选择特征的代码如下：

```python
from sklearn.feature_selection import SelectKBest
from scipy.stats import pearsonr

#选择K个最好的特征，返回选择特征后的数据
#第一个参数为计算评估特征是否好的函数，该函数输入特征矩阵和目标向量，输出二元组（评分，P值）的数组，数组第i项为第i个特征的评分和P值。在此定义为计算相关系数
#参数k为选择的特征个数
SelectKBest(lambda X, Y: array(map(lambda x:pearsonr(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



4.1.3 卡方检验

经典的卡方检验是检验定性自变量对定性因变量的相关性。假设自变量有N种取值，因变量有M种取值，考虑自变量等于i且因变量等于j的样本频数的观察值与期望的差距，构建统计量：

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151972.png)

```python
 from sklearn.feature_selection import SelectKBest
 from sklearn.feature_selection import chi2
 
 #选择K个最好的特征，返回选择特征后的数据
 SelectKBest(chi2, k=2).fit_transform(iris.data, iris.target)
```



4.1.4  互信息法

经典的互信息也是评价定性自变量对定性因变量的相关性的，互信息计算公式如下：

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151973.png)

```python
from sklearn.feature_selection import SelectKBest
from minepy import MINE

#由于MINE的设计不是函数式的，定义mic方法将其为函数式的，返回一个二元组，二元组的第2项设置成固定的P值0.5
def mic(x, y):
    m = MINE()
    m.compute_score(x, y)
    return (m.mic(), 0.5)

#选择K个最好的特征，返回特征选择后的数据
SelectKBest(lambda X, Y: array(map(lambda x:mic(x, Y), X.T)).T, k=2).fit_transform(iris.data, iris.target)
```



#### 4.2  wrapper

4.2.1  递归特征消除法

递归消除特征法使用一个基模型来进行多轮训练，每轮训练后，消除若干权值系数的特征，再基于新的特征集进行下一轮训练。

使用feature_selection库的RFE类来选择特征的代码如下：

```python
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression

#递归特征消除法，返回特征选择后的数据
#参数estimator为基模型
#参数n_features_to_select为选择的特征个数
RFE(estimator=LogisticRegression(), n_features_to_select=2).fit_transform(iris.data, iris.target)
```



#### 4.3  embedded

4.3.1  基于惩罚项的特征选择法

```python
 from sklearn.feature_selection import SelectFromModel
 from sklearn.linear_model import LogisticRegression
 
 #带L1惩罚项的逻辑回归作为基模型的特征选择
 SelectFromModel(LogisticRegression(penalty="l1", C=0.1)).fit_transform(iris.data, iris.target)
```



4.3.2  基于树模型的特征选择法

树模型中GBDT也可用来作为基模型进行特征选择，使用feature_selection库的SelectFromModel类结合GBDT模型，来选择特征的代码如下：

```python
 from sklearn.feature_selection import SelectFromModel
 from sklearn.ensemble import GradientBoostingClassifier
 
 #GBDT作为基模型的特征选择
 SelectFromModel(GradientBoostingClassifier()).fit_transform(iris.data, iris.target)
```











### 5、降维

### （1）主成分分析

- 本质 : PCA是一种分析,简化数据集的技术.
- 思想 : 将n维特征映射到k维上，这k维是全新的正交特征也被称为主成分，是在原有n维特征的基础上重新构造出来的k维特征
- 目的 : 是数据维度压缩,尽可能降低原数据的维数(复杂度),尽可能的减少损失信息

- 作用 : 可以消减回归分析或者聚类分析中特征的数量

- 使用场景 : 特征数量达到上百的时候,考虑数据的简化.

步骤：

1. 去中心化（把坐标原点放在数据的中心）
2. 找坐标系（找到方差最大的方向）



```python
类 sklearn.decompositon.PCA
PCA(n_componets=None) 
    将数据分解为较低维数据 
    PCA.ﬁt_transform(x) 
        x:numpy array格式 
        返回值：转换后降低维度的array 
    n_componets参数： 
        小数：表示将信息保存到原信息的百分比，例如0.95表示降维后信息量是原来的95%。一般制定到0.9-0.95 
        整数：较少到的特征数量，一般不使用 
    流程
        实例化 PCA 
        调用 ﬁt_transform
```

```python
from sklearn.decomposition import PCA
pca = PCA(n_components=0.95)
data = np.array([[2,8,4,5],[6,3,0,8],[5,4,9,1]])
print('主成分分析降维前:\n',data)
res = pca.fit_transform(data)
print('主成分分析降维后:\n',res)
# 维度下降,不损失信息
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151974.png)









### （2）线性判别分析法

```python
 from sklearn.lda import LDA
 
 #线性判别分析法，返回降维后的数据
 #参数n_components为降维后的维数
 LDA(n_components=2).fit_transform(iris.data, iris.target)
```



### 6、调整参数

- 首先，将全部样本划分成k个大小相等的样本子集；
- 依次遍历这k个子集，每次把当前子集作为验证集，其余所有样本作为训练集，进行模型的训练和评估；
- 最后把k次评估指标的平均值作为最终的评估指标。在实际实验中，k通常取10.

```python
sklearn.model_selection.cross_val_score(estimator, X, y=None, groups=None, scoring=None, cv=’warn’, n_jobs=None, verbose=0, fit_params=None, pre_dispatch=‘2*n_jobs’, error_score=’raise-deprecating’)
```

estimator： 需要使用交叉验证的算法
X： 输入样本数据
y： 样本标签
groups： 将数据集分割为训练/测试集时使用的样本的组标签（一般用不到）
scoring： 交叉验证最重要的就是他的验证方式，选择不同的评价方法，会产生不同的评价结果。具体可用哪些评价指标，官方已给出详细解释

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151976.png)

```python

for n in k_range:
    knn = KNeighborsClassifier(n)   #knn模型，这里一个超参数可以做预测，当多个超参数时需要使用另一种方法GridSearchCV
    scores = cross_val_score(knn,train_X,train_y,cv=10,scoring='accuracy')  #cv：选择每次测试折数  accuracy：评价指标是准确度,可以省略使用默认值，具体使用参考下面。
    cv_scores.append(scores.mean())
plt.plot(k_range,cv_scores)
plt.xlabel('K')
plt.ylabel('Accuracy')		#通过图像选择最好的参数
plt.show()
```













## 常用算法

### 1、KNN算法

kNN模型的三个基本要素：（1）距离度量、（2）k值的选择、（3）分类决策规则。

（1）距离度量

在引例中所画的坐标系，可以叫做特征空间。特征空间中两个实例点的距离是两个实例点相似程度的反应（距离越近，相似度越高）。kNN模型使用的距离一般是欧氏距离，但也可以是其他距离如：曼哈顿距离

（2）k值的选择

k值的选择会对kNN模型的结果产生重大影响。选择较大的k值，相当于用较大邻域中的训练实例进行预测，模型会考虑过多的邻近点实例点，甚至会考虑到大量已经对预测结果没有影响的实例点，会让预测出错；选择较小的k值，相当于用较小邻域中的训练实例进行预测，会使模型变得敏感（如果邻近的实例点恰巧是噪声，预测就会出错）。

在应用中，k值一般取一个比较小的数值。通常采用交叉验证法来选取最优的k值。

（3）分类决策规则

kNN中的分类决策规则往往是多数表决，即由输入实例的k个邻近的训练实例中的多数类决定待测实例的类。



```
1 计算已知类别中数据集的点与当前点的距离。[即计算所有样本点跟待分类样本之间的距离]
2 按照距离递增次序排序。[计算完样本距离进行排序]
3 选取与当前点距离最小的k个点。[选取距离样本最近的k个点]
4 确定前k个点所在类别的出现频率。[针对这k个点，统计下各个类别分别有多少个]
5 返回前k个点出现频率最高的类别作为当前点的预测分类。[k个点中某个类别最多，就将样本划归改点]
```



```python
# -*- coding: utf-8 -*-
# 导入鸢尾花数据集
from sklearn.datasets import load_iris
# 导入knn算法
from sklearn import neighbors

# 加载数据
iris = load_iris()
# 提取数据
trainX = iris.data
trainY = iris.target

# 建立模型
clf = neighbors.KNeighborsClassifier(n_neighbors=6,
                                     weights='uniform', algorithm='auto', leaf_size=30,
                                     p=2, metric='minkowski', metric_params=None, n_jobs=1)
'''
    @param n_neighbors: 指定kNN的k值
    @param weights:  
    'uniform': 本节点的所有邻居节点的投票权重都相等
    'distance': 本节点的所有邻居节点的投票权重与距离成反比
    @param algorithm:  惩罚项系数的倒数，越大，正则化项越小
    'ball_tree': BallTree算法
    'kd_tree': kd树算法
    'brute': 暴力搜索算法
    'auto': 自动决定适合的算法
    @param leaf_size:  指定ball_tree或kd_tree的叶节点规模。他影响树的构建和查询速度
    @param p:  p=1:曼哈顿距离; p=2:欧式距离
    @param metric:  指定距离度量，默认为'minkowski'距离
    @param n_jobs: 任务并行时指定使用的CPU数，-1表示使用所有可用的CPU

    @method fit(X,y): 训练模型
    @method predict(X): 预测
    @method score(X,y): 计算在(X,y)上的预测的准确率
    @method predict_proba(X): 返回X预测为各类别的概率
    @method kneighbors(X, n_neighbors, return_distance): 返回样本点的k近邻点。如果return_distance=True，则也会返回这些点的距离
    @method kneighbors_graph(X, n_neighbors, mode): 返回样本点的连接图
'''
# 训练模型
clf.fit(trainX, trainY)
# 打印准确率
print("训练准确率:" + str(clf.score(trainX, trainY)))

print("测试准确率:" + str(clf.score(trainX, trainY)))


'''
训练准确率:0.973333333333
测试准确率:0.973333333333
'''

```



优点：

- 训练时间复杂度为(n)
- 既可以用来做分类也可以用来做回归
- 理论简单，容易实现，可用于非线性分类



缺点：

- 内存消耗大
- k值大小需要预先设定，不能自适应
- 样本不平衡时，偏差比较大







### 2、贝叶斯算法

**优点**

- 朴素贝叶斯对结果解释容易理解
-  所需估计的参数少， 对于缺失数据不敏感

**缺点**

- 假设属性之间相互独立，这往往并不成立
- 需要知道先验概率

#### （1）GaussianNB （高斯朴素贝叶斯）

![f(x_{i}|y)=\frac{1}{\sqrt{2\pi\sigma_{y}^{2} }}exp(-\frac{(x_{i}-\mu _{y})^{2}}{2\sigma_{y}^{2}} )](https://raw.githubusercontent.com/a1254898873/images/master/202203241151977.png)

这种模型假设特征符合高斯分布。由概率密度函数可知，这个模型适用于数值型特征。

在贝叶斯分类中，高斯模型就是用来处理连续型特征变量的，当使用此模型时，我们会假定特征属于高斯分布，然后基于训练样本计算特征均值和标准差，这样就可以得到此特征下每一个属性值的先验概率。

在Sklearn库中高斯模型对应于GaussianNB()，只有一个参数：先验概率priors，即类别的先验概率， 如果不给则直接用类别数/总数得到类别概率

使用GaussianNB()拟合后，可以调用3个方法
①predict:与决策树一样，直接给出分类结果
②predict_proba: 给出每一个测试集样本属于每个类别的概率，最大的就是分类结果
③ predict_log_proba: predict_proba的对数转化，最大的就是分类结果

```python
from sklearn import naive_bayes
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn import metrics

# 导入数据
data,target = datasets.load_iris(return_X_y=True)
print(data.shape,target.shape)
# 拆分为训练集和测试集
x_train,x_test,y_train,y_test = train_test_split(data,target,train_size=0.3)

# 高斯模型
clf = naive_bayes.GaussianNB()
clf.fit(x_train,y_train)
y_pred = clf.predict(x_test)
y_pred_proba = clf.predict_proba(x_test)
y_pred_log_proba = clf.predict_log_proba(x_test)
print(y_pred[:5])
print(y_pred_proba[:5])
print(y_pred_log_proba[:5])
print('训练集评分:', clf.score(x_train,y_train))
print('测试集评分:', clf.score(x_test,y_test))
print("精准率:", metrics.accuracy_score(y_test,y_pred))
```



#### （2）多项式分布贝叶斯 （MultinomialNB）

​	![\widehat{\theta}_{yi}=\frac{N_{yi}+\alpha }{N_{y}+\alpha n}](https://raw.githubusercontent.com/a1254898873/images/master/202203241151978.png)

这个模型假设特征复合多项式分布，是一种非常典型的文本分类模型，用来处理离散型特征变量的。模型内部带有平滑参数\alpha。

Sklearn库中对应于 MultinomialNB()，有3个参数
① alpha：常数λ，如果你没有特别的需要，用默认的1即可。如果发现拟合的不好，需要调优时，可以选择稍大于1或者稍小于1的数。
②fit_prior：表示是否要考虑先验概率，如果是false, 则所有的样本类别输出都有相同的类别先验概率。否则可以自己用第三个参数class_prior输入先验概率，或者不输入第三个参数class_prior让MultinomialNB自己从训练集样本来计算先验概率，此时的先验概率为P(Y=Ck)=mk/m。其中m为训练集样本总数量，mk为输出为第k类别的训练集样本数。

```python
class sklearn.naive_bayes.MultinomialNB(alpha=1.0, fit_prior=True, class_prior=None)
```



#### （3）伯努利模型（BernoulliNB）

![P(x_{i}|y)=P(i|y)x_{i}+(1-P(i|y))(1-x_{i})](https://raw.githubusercontent.com/a1254898873/images/master/202203241151979.png)

用来处理特征变量是布尔类型变量，伯努利模型对应的是伯努利分布，是一种只有“是"或"否"两种结果的随机变量分布，比如抛硬币的正或反就是非常典型的伯努利分布。

在伯努利模型中，所有特征的取值都会变成0或1，如果特征本身并不是0或1，则会自动设定某个阈值，将低于阈值地设为0，高于阈值地设为1，从而将特征0-1化。然后在所有训练样本中计算0-1的概率，作为特征的条件概率

Sklearn库中对应于 BernoulliNB()，有4个参数，前3个与多项式一样，唯一增加的一个参数是binarize，可以是数值或者不输入。如果不输入，则BernoulliNB认为每个数据特征都已经是二元的。否则的话，小于binarize的会归为一类，大于binarize的会归为另外一类


```python
class sklearn.naive_bayes.BernoulliNB(alpha=1.0, binarize=0.0, fit_prior=True, class_prior=None)
```



### 3、决策树

**步骤:**

（1）特征选择

特征选择决定了使用哪些特征来做判断。在训练数据集中，每个样本的属性可能有很多个，不同属性的作用有大有小。因而特征选择的作用就是筛选出跟分类结果相关性较高的特征，也就是分类能力较强的特征。

在特征选择中通常使用的准则是：信息增益，信息增益比。

信息增益对应的是ID3，信息增益比对应的是C4.5

信息增益会偏向取值更多的属性，而信息增益比不会

（2）决策树生成

选择好特征后，就从根节点触发，对节点计算所有特征的信息增益，选择信息增益最大的特征作为节点特征，根据该特征的不同取值建立子节点；对每个子节点使用相同的方式生成新的子节点，直到信息增益很小或者没有特征可以选择为止。

（3）决策树剪枝

剪枝的主要目的是对抗“过拟合”，通过主动去掉部分分支来降低过拟合的风险。



**原理:**

```
'''
决策树工作原理：基于迭代的思想。
'''
def createBranch():
    检测数据集中的所有数据的分类标签是否相同:
        If so return 类标签
        Else:
            寻找划分数据集的最好特征（划分之后信息熵最小，也就是信息增益最大的特征）
            划分数据集
            创建分支节点
                for 每个划分的子集
                    调用函数 createBranch （创建分支的函数）并增加返回结果到分支节点中
            return 分支节点
```





**实现**

决策树算法有：ID3，C4.5，CART

ID3，C4.5是分类树，CART是回归树

sklearn中训练决策树的默认算法是CART，使用CART决策树的好处是可以用它来进行回归和分类处理

```python
from sklearn.tree import DecisionTreeClassifier


# 训练模型，限制树的最大深度4
clf = DecisionTreeClassifier(max_depth=4)
#拟合模型
clf.fit(X, y)

```



**优点**

- 适合高维度数据
- 短时间内处理大量数据， 得到可行且效果较好的效果。
- 简单，易于理解



**缺点**

- 易于过拟合
-  对于各类别样本数量不一致数据， 信息增益偏向于那些具有更多数值的特征
- 忽略属性之间的相关性







### 4、SVM算法

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151980.jpg" alt="img" style="zoom:50%;" />

支持向量机（support vector machines, SVM）是一种二分类模型，它的基本模型是定义在特征空间上的间隔最大的线性分类器，间隔最大使它有别于感知机；SVM还包括核技巧，这使它成为实质上的非线性分类器。SVM的的学习策略就是间隔最大化，可形式化为一个求解凸二次规划的问题，也等价于正则化的合页损失函数的最小化问题。SVM的的学习算法就是求解凸二次规划的最优化算法。

SVM学习方法的选择：

- 当训练数据线性可分时，通过硬间隔最大化学习一个线性分类器,即线性可分支持向量机又称为硬间隔支持向量机。
- 当训练数据近似线性可分时，通过软间隔最大化学习一个线性分类器,即线性可分支持向量机又称为软间隔支持向量机。
- 当训练数据线性不可分时，通过核函数及软间隔最大化学习一个非线性支持向量机。



松弛变量：我们知道几乎所有的数据都不那么干净, 通过引入松弛变量来 允许数据点可以处于分隔面错误的一侧。

核函数(kernel) ：对于线性可分的情况，效果明显,对于非线性的情况也一样，此时需要用到一种叫核函数(kernel)的工具将数据转化为分类器易于理解的形式。利用核函数将数据映射到高维空间,使用核函数可以将数据从某个特征空间到另一个特征空间的映射。最流行的核函数：径向基函数(radial basis function).



间隔最大化：

![equation](https://raw.githubusercontent.com/a1254898873/images/master/202203241151981.svg)

我们的目标是使margin最大

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151982.svg)

加上约束条件

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151983.svg)

间隔最大化数学表达式

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151984.svg)



拉格朗日乘子法
$$
{\mathcal {L}}(x,y,\lambda )=f(x,y)+\lambda \cdot {\Big (}g(x,y)-c{\Big )}
$$
我们称式（2.2.4）所述问题为原始问题(primal problem), 可以应用拉格朗日乘子法构造拉格朗日函数(Lagrange function)再通过求解其对偶问题(dual problem)得到原始问题的最优解。



```python
from sklearn.svm import SVC
classifier = SVC(kernel = 'linear', random_state = 0)
classifier.fit(X_train, y_train)
```



**优点**

- 可以解决高维、 非线性问题
- 泛化能力比较强




**缺点**

- 运行速度慢，调参复杂
- 内存消耗大





### 5、回归算法

### （1）线性回归

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151985.png)

```
每个回归系数初始化为 1
重复 R 次:
    计算整个数据集的梯度
    使用 步长 x 梯度 更新回归系数的向量
返回回归系数
```



```python
from sklearn.linear_model import LinearRegression
regressor = LinearRegression()
regressor = regressor.fit(X_train, Y_train)
```

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151986.gif)

在机器学习中，通常损失函数会加上一个额外项，可看作损失函数的惩罚项，是为了限制模型参数，防止过拟合。

损失函数为L1范数的是Lasso回归，损失函数是L2范数的是岭回归。





### （2）逻辑回归

逻辑回归用来处理分类问题，将线性函数的结果映射到sigmoid函数中

```python
from sklearn.linear_model import LogisticRegression
regressor = LogisticRegression()
regressor.fit(X_train, Y_train)
```

**优点**

- 速度快
- 简单、易于理解

**缺点**

- 特征特征处理过程复杂
- 容易欠拟合



### 6、k-means聚类算法

K-Means 是发现给定数据集的 K 个簇的聚类算法, 之所以称之为 K-均值 是因为它可以发现 K 个不同的簇, 且每个簇的中心采用簇中所含值的均值计算而成.簇个数 K 是用户指定的, 每一个簇通过其质心（centroid）, 即簇中所有点的中心来描述.

聚类与分类算法的最大区别在于, 分类的目标类别已知, 而聚类的目标类别是未知的.

步骤：

　　(1)从数据中选择k个对象作为初始聚类中心;

　　(2)计算每个聚类对象到聚类中心的距离来划分；

　　(3)再次计算每个聚类中心

　　(4)计算标准测度函数，之道达到最大迭代次数，则停止，否则，继续操作。



```
创建 k 个点作为起始质心（通常是随机选择）
当任意一个点的簇分配结果发生改变时（不改变时算法结束）
    对数据集中的每个数据点
        对每个质心
            计算质心与数据点之间的距离
        将数据点分配到距其最近的簇
    对每一个簇, 计算簇中所有点的均值并将均值作为质心
```



```python
from sklearn.cluster import KMeans
y_pred = KMeans(n_clusters=2, random_state=9).fit_predict(X)
```



### 7、Adaboost 

Boosting是一种集合技术，试图从许多弱分类器中创建一个强分类器。这是通过从训练数据构建模型，然后创建第二个模型来尝试从第一个模型中纠正错误来完成的。添加模型直到完美预测训练集或添加最大数量的模型。

AdaBoost是第一个为二进制分类开发的真正成功的增强算法。这是理解助力的最佳起点。现代助推方法建立在AdaBoost上，最着名的是随机梯度增强机。

![boosting的具体过程](https://raw.githubusercontent.com/a1254898873/images/master/202203241151987.png)



```python
#设定弱分类器CART，每个树的最大深度设置为2
weakClassifier=DecisionTreeClassifier(max_depth=2)

clf=AdaBoostClassifier(base_estimator=weakClassifier,algorithm='SAMME',n_estimators=300,learning_rate=0.8)
clf.fit(X, y)
```

理论上任何学习器都可以用于Adaboost.但一般来说，使用最广泛的Adaboost弱学习器是决策树和神经网络。对于决策树，Adaboost分类用了CART分类树，而Adaboost回归用了CART回归树。上面代码中使用的弱分类器就是CART分类决策树。





### 8、随机森林

一棵树是决策树，多棵树是随机森林，解决了决策树泛化能力弱的缺点。

（1）预设模型的超参数（几个树？分几层？）

（2）随机采样，训练每个决策树

（3）输入dai待检测样本到每个树中，再将每个树的结果整合

回归：求均值            分类：求众数

```python
from sklearn.tree import RandomForestClassifier     #导入需要的模块

rfc = RandomForestClassifier()                      #实例化
rfc = rfc.fit(X_train,y_train)                      #用训练集数据训练模型
result = rfc.score(X_test,y_test)                   #导入测试集，从接口中调用需要的信息
```









# 深度学习

## 发展史

2006年Hinton等人在science期刊上发表了论文“Reducing the dimensionality of data with neural networks”，揭开了新的训练深层神经网络算法的序幕。利用无监督的RBM网络来进行预训练，进行图像的降维，取得比PCA更好的结果，通常这被认为是深度学习兴起的开篇。



2009年，ImageNet数据集被整理，并于次年开始每年举办一次比赛。ImageNet 数据集总共有1400多万幅图片，涵盖2万多个类别，为计算机视觉领域做出了巨大的贡献，至今我们仍然使用着Imagenet来评估算法，以及预训练其他任务的模型。



2011年，Glorot等人提出ReLU激活函数，有效地抑制了深层网络的梯度消失问题，现在最好的激活函数都是来自于ReLU家族，简单而有效。



2012年，Hinton的学生Alex Krizhevsky提出AlexNet网络，以低于第2名10%的错误率赢得了ImageNet竞赛。当时Alex Krizhevsky使用了两块显卡GTX580，花了6天时间才训练出AlexNet，我相信如果有更多的资源，AlexNet一定是一个更好的AlexNet。



2014年，GoogLeNet和VGGNet分别被提出，获得ImageNet分类赛的冠亚军。VGGNet很好的展示了如何在先前网络架构的基础上通过简单地增加网络层数和深度就可以提高网络的性能，GoogleNet模型架构则提出了Inception结构，拓宽神经的宽度，成为了计算效率较高的深层模型基准之一。



2014年，无监督学习网络GAN横空出世，独立成了一个新的研究方向，被LeCun誉为下一代深度学习，此后GAN在各大领域，尤其是图像领域不断“建功立业”，并与各类CNN网络结构进行了融合。



2015年，ResNet获得了ImageNet2012分类任务冠军，以3.57%的错误率表现超过了人类的识别水平，并以152层的网络架构创造了新的模型记录，自此残差连接在CNN的设计中随处可见。





## 激活函数

正如其名字所表明，神经网络的灵感来源于人类大脑的神经结构，像在一个人类大脑中，最基本的构件就叫做神经元。它的功能和人的神经元很相似，换句话说，它有一些输入，然后给一个输出。在数学上，在机器学习中的神经元就是一个数学函数的占位符，它仅有的工作就是对输入使用一个函数，然后给一个输出。

![理解神经网络：从神经元到RNN、CNN、深度学习](https://raw.githubusercontent.com/a1254898873/images/master/202203241151988.png)

这种神经元中使用的函数，在术语上通常叫做激活函数。如果没有激活函数，增加网络模型仍然是线性的。

激活函数性质：

1、可微性：当优化方法是基于梯度的时候，这个性质是必须的；

2、单调性：当激活函数是单调函数的时候，单层网络能够保证是凸函数；

3、输出值的范围：当激活函数输出值是有限的时候，基于梯度的优化方法会更加稳定。



### （1）阶跃函数

![理解神经网络：从神经元到RNN、CNN、深度学习](https://raw.githubusercontent.com/a1254898873/images/master/202203241151989.png)

其中，如果x的值大于等于零，则输出为1；如果x的值小于零，则输出为0。我们可以看到阶跃函数在零点是不可微的。目前，神经网络采用反向传播法和梯度下降法来计算不同层的权重。由于阶跃函数在零处是不可微的，因此它并不适用于梯度下降法，并且也不能应用在更新权重的任务上。

为了克服这个问题，我们引入了sigmoid函数。



### （2）Sigmoid函数

![理解神经网络：从神经元到RNN、CNN、深度学习](https://raw.githubusercontent.com/a1254898873/images/master/202203241151990.png)

当z或自变量趋于负无穷大时，函数的值趋于零；当z趋于正无穷大时，函数的值趋于1。需要记住的是，该函数表示因变量行为的近似值，并且是一个假设。现在问题来了，为什么我们要用Sigmoid函数作为近似函数之一。这有一些简单的原因。

1. 它在可以捕获数据的非线性。虽然是一个近似的形式，但非线性的概念是模型精确的重要本质。

2. sigmoid函数在整个过程中是可微的，因此可以与梯度下降和反向传播方法一起使用，以计算不同层的权重。

3. 假设一个因变量服从一个sigmoid函数的固有假设的高斯分布的自变量，这是一个一般分布，我们可以获得许多随机发生的事件，这是一个好的的一般分布开始。

然而，sigmoid函数也面临着梯度消失的问题。从图中可以看出，一个sigmoid函数将其输入压缩到一个非常小的输出范围[0,1]，并具有非常陡峭的渐变。因此，输入空间中仍然有很大的区域，即使是很大的变化也会在输出中产生很小的变化。这被称为梯度消失问题。这个问题随着层数的增加而增加，从而使神经网络的学习停留在一定的水平上。



### （3）Tanh函数

Tanh(z)函数是sigmoid函数的缩放版本，它的输出范围变成了[-1,1]，而不是[0,1].

![理解神经网络：从神经元到RNN、CNN、深度学习](https://raw.githubusercontent.com/a1254898873/images/master/202203241151991.png)



### （4）ReLU函数

在深度学习模型中，修正线性单元(ReLU)是最常用的激活函数。当函数输入负数时，函数输出0，对于任意正数x，函数输出本身。因此它可以写成f(x)=max(0,x)

![理解神经网络：从神经元到RNN、CNN、深度学习](https://raw.githubusercontent.com/a1254898873/images/master/202203241151992.png)



### （5）LeakyReLU

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203282041146.png)

LeakyReLU的提出就是为了解决神经元”死亡“问题，LeakyReLU与ReLU很相似，仅在输入小于0的部分有差别，ReLU输入小于0的部分值都为0，而LeakyReLU输入小于0的部分，值为负，且有微小的梯度。

```python
nn.LeakyReLU(0.02)
```



### （6）softmax函数

![image-20220321095655919](https://raw.githubusercontent.com/a1254898873/images/master/202203241151993.png)

Softmax 函数具体的作用是将输入标准化到和为 1 的输出，经过 Softmax 函数的的数据可以被认为是概率。适合于多分类问题。

```python
def softmax(input):
    assert len(input.shape) == 2 # 输入 softmax 函数的数据必须是一个二维矩阵
    exp_value = np.exp(input)  # 首先计算指数
    output = exp_value/np.sum(exp_value, axis=1)[:, np.newaxis]  # 然后按行标准化
    return output
```





## 损失函数

### （1）交叉熵

假设交叉熵损失函数为  𝐿 ，那么单个样本的损失定义为:
$$
L _ { i } = - Y_i \log P_i \tag{6}
$$
指的是在样本是 $x_i$ 的情况下，使用概率分布 $P_i$ 拟合真实概率分布 $Y_i$ 的误差。

多个样本时，只需要将各个样本的损失相加即可:
$$
L = \frac{1}{N}\sum L _ { i } \tag{8}
$$

```python
loss = np.sum(-np.log(prob) * test_labels) / N
```















## 梯度下降

![image-20220315201354126](https://raw.githubusercontent.com/a1254898873/images/master/202203241151994.png)

一维度的标量x的梯度就是算f(x)对x的微分，多维度的向量x的梯度就是算f(x)对x所有元素的偏微分

 梯度下降法是一种不断去更新参数(这边参数用x表示)找「解」的方法，所以一定要先随机产生一组初始参数的「解」，然后根据这组随机产生的「解」开始算此「解」的梯度方向大小，然后将这个「解」去减去梯度方向

<img src="https://blog.paperspace.com/content/images/2018/06/patho2-1.png" alt="patho2-1" style="zoom:80%;" />

<img src="https://blog.paperspace.com/content/images/2018/06/patho3.png" alt="patho3" style="zoom:80%;" />



梯度下降沿着山沟的山脊反弹，向极小的方向移动较慢。这是因为脊的表面在W1方向上弯曲得更陡峭。

考虑在脊的表面上的一个点梯度。该点的梯度可以分解为两个分量，一个沿着方向w1，另一个沿着w2。梯度在w1方向上的分量要大得多，因此梯度的方向更靠近w1，而不是朝向w2（最小值位于其上）。

通常情况下，我们使用低学习率来应对这样的反复振荡，但在病态曲率区域使用低学习率，可能要花很多时间才能达到最小值处。事实上，有论文报告，防止反复振荡的足够小的学习率，也许会导致从业者相信损失完全没有改善，干脆放弃训练。

大概，我们需要找到一种方法，首先缓慢地进入病态曲率的平坦底部，然后加速往最小值方向移动。二阶导数可以帮助我们来到来到赖这一点一一。

### （1）牛顿法

下降梯度的英文一阶优化方法。它只考虑损失函数的一阶导数而不是较高的导数。这基本上意味着它没有关于损失函数曲率的线索。它可以判断损失是否在下降和速度有多快，但无法区分曲线是平面，向上弯曲还是向下弯曲。

<img src="https://blog.paperspace.com/content/images/2018/06/firstorder.png" alt="ç¬¬ä¸ä¸ªè®¢å" style="zoom:50%;" />

上图三条曲线，红点处的梯度都是一样的，但曲率大不一样。解决方案？考虑二阶导数，或者说梯度改变得有多快。

使用二阶导数解决这一问题的一个非常流行的技术是牛顿法（Newton's Method）。

但牛顿法不经常使用，因为现代神经网络架构的参数量可能是数亿，计算数亿的平方的梯度在算力上不可行。



### （2）随机梯度下降（SGD）

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151995.png)

SGD就是一次跑一个样本或是小批次(mini-batch)样本然后算出一次梯度或是小批次梯度的平均后就更新一次，那这个样本或是小批次的样本是随机抽取的，所以才会称为随机梯度下降法。

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151996.png" alt="image-20220319200827362" style="zoom:67%;" />

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151997.png" alt="image-20220319200906929" style="zoom:67%;" />

　　 SGD存在两个问题：

　　　　(1) SGD方法中的高方差振荡使得网络很难稳定收敛，即会之字形摇摆，收敛速度慢。

　　　　(2)可能会陷入局部最小值而无法跳出。



### （3）动量法（Momentum）

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151998.png" alt="image-20220319201533032" style="zoom:67%;" />

冲量梯度下降算法是Boris Polyak在1964年提出的，其基于这样一个物理事实：将一个小球从山顶滚下，其初始速率很慢，但在加速度作用下速率很快增加，并最终由于阻力的存在达到一个稳定速率。对于冲量梯度下降算法，其更新方程如下：

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151999.svg)

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151000.svg)

动量等式由两部分组成，第一项是上一次迭代的动量，乘以“动量系数”。

可以看到，参数更新时不仅考虑当前梯度值，而且加上了一个积累项（冲量），但多了一个超参Cgamma，一般取接近1的值如0.9。相比原始梯度下降算法，冲量梯度下降算法有助于加速收敛。当梯度与冲量方向一致时，冲量项会增加，而相反时，冲量项减少，因此冲量梯度下降算法可以减少训练的震荡过程。

有时候，冲量梯度下降算法也可以按下面方式实现：

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151001.svg)

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151002.svg)

所谓的冲量项其实只是梯度的指数加权移动平均值。这个实现和之前的实现没有本质区别，只是学习速率进行了放缩一下而已。



```python
from torch import optim
optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)  #model.parameters()， 神经网络的权重参数
```

 pytorch的optim.SGD()是SDG+动量的方法



### （4）AdaGrad（自适应梯度算法）

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151003.png" alt="image-20220319204039171" style="zoom:67%;" />

![[公式]](https://www.zhihu.com/equation?tex=s%5Cleftarrow+s+%2B+%5Cnabla+J%28%5Ctheta%29%5Codot+%5Cnabla+J%28%5Ctheta%29)

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151004.svg)

AdaGrad其学习速率实际上是不断衰减的，这会导致一个很大的问题，就是训练后期学习速率很小，导致训练过早停止，因此在实际中AdaGrad一般不会被采用.

```python
d = (torch.randn(5, 3) for i in range(3))
optimizer = optim.Adagrad(d, lr=0.001)
print(optimizer)
```



### （5）RMSprop

是对Adagrad算法的改进，主要是解决学习速率过快衰减的问题。其实思路很简单，类似Momentum思想，引入一个超参数，在积累梯度平方项进行衰减：

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151005.svg)

![[公式]](https://raw.githubusercontent.com/a1254898873/images/master/202203241151006.svg)

```python
params = (torch.randn(5, 3) for i in range(3))
optimizer = optim.RMSprop(params, lr=0.01, alpha=0.99, eps=1e-08, weight_decay=0, momentum=0, centered=False)
print(optimizer)
```



### （6）Adam（自适应动量优化）

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151007.png" alt="image-20220319204749032" style="zoom: 67%;" />

Adam是Kingma等在2015年提出的一种新的优化算法，其结合了Momentum和RMSprop算法的思想。相比Momentum算法，其学习速率是自适应的，而相比RMSprop，其增加了冲量项。所以，Adam是两者的结合体

```python
params = (torch.randn(5, 3) for i in range(3))
optimizer = optim.Adam(params, lr=0.001, betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=Fals)
print(optimizer)
```









## 反向传播

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203262208103.png)

反向传播是一种与最优化方法（如梯度下降法）结合使用的，用来训练人工神经网络的常见方法。该方法对网络中所有权重计算损失函数的梯度。这个梯度会反馈给最优化方法，用来更新权值以最小化损失函数。

反向传播这个术语经常被误解为用于多层神经网络的整个学习算法。实际上，反向传播仅指用于计算梯度的方法。而另一种算法，例如随机梯度下降法，才是使用该梯度来进行学习。

反向传播是一种计算链式法则的算法，使用高效的特定运输顺序。

主要思想是：

- 将训练集数据输入到ANN的输入层，经过隐藏层，最后达到输出层并输出结果，这是ANN的前向传播过程；
- 由于ANN的输出结果与实际结果有误差，则计算估计值与实际值之间的误差，并将该误差从输出层向隐藏层反向传播，直至传播到输入层；
- 在反向传播的过程中，根据误差调整各种参数的值；不断迭代上述过程，直至收敛。



### 1、变量定义

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241257534.png" alt="img" style="zoom:50%;" />



![image-20220324125823822](https://raw.githubusercontent.com/a1254898873/images/master/202203241258936.png)









### 2、代价函数

![image-20220324130144088](https://raw.githubusercontent.com/a1254898873/images/master/202203241301188.png)





### 3、公式及其推导

![image-20220324130517830](https://raw.githubusercontent.com/a1254898873/images/master/202203241305926.png)

公式1（计算最后一层神经网络产生的错误）：

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241309298.png" alt="image-20220324130915197" style="zoom:67%;" />

```python
                layer.error = y - output
                # 计算最后一层的 delta，参考输出层的梯度公式
                layer.delta = layer.error * layer.apply_activation_derivative(output)
```



公式2（由后往前，计算每一层神经网络产生的错误）：

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241312371.png" alt="image-20220324131214266" style="zoom:67%;" />

```python
                next_layer = self._layers[i + 1]
                layer.error = np.dot(next_layer.weights, next_layer.delta)
                layer.delta = layer.error*layer.apply_activation_derivative(layer.activation_output)
```



公式3（计算权重的梯度）：

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241313718.png" alt="image-20220324131335621" style="zoom:67%;" />

公式4（计算偏置的梯度）：



<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241322662.png" alt="image-20220324132206569" style="zoom:67%;" />







## 正则化方法

### 1、DropOut

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203292224896.png)

在训练阶段，对于每个节点，以p概率将其输出值保留，以1-p概率将其输出值乘以0

在测试阶段，输出结果要乘以p。原因是：保持训练阶段和测试阶段的期望值相同。训练阶段，对于每个节点，dropout之前的输出是x, 经历dropout之后的期望值是px+(1−p)∗0=px，因此在测试阶段需要将结果乘以p，从而输出的期望值是px。


加入dropout可以防止过拟合

```python
nn.Dropout(p），其中p是采样概率
```

```python
    def forward(self, x):
        out = self.fc1(x)
        out = self.dropout(out)
        out = self.relu(out)
        out = self.fc2(out)
        return out
```





### 2、DropBlock

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203292229514.png)

(a)原始输入图像

(b)绿色部分表示激活的特征单元，b图表示了随机dropout激活单元，但是这样dropout后，网络还会从drouout掉的激活单元附近学习到同样的信息

(c)绿色部分表示激活的特征单元，c图表示本文的DropBlock，通过dropout掉一部分相邻的整片的区域（比如头和脚），网络就会去注重学习狗的别的部位的特征，来实现正确分类，从而表现出更好的泛化。





### 3、DropConnect

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203292234835.png)

DropOut是将神经元激活输出随机置0，而DropConnect是作用于权重，以1-p概率将节点中的每个与其相连的输入权重清0





### 4、损失函数加惩罚项

torch.optim集成了很多优化器，如SGD，Adadelta，Adam，Adagrad，RMSprop等，这些优化器自带的一个参数weight_decay，用于指定权值衰减率，相当于L2正则化中的λ参数，注意torch.optim集成的优化器只有L2正则化方法

```python
 optimizer = optim.Adam(model.parameters(),lr=learning_rate,weight_decay=0.01)
```









## 归一化方法

作用：

（1）减轻了对参数初始化的依赖，前向激活值和反向梯度更加有效

（2）平滑了优化目标函数的曲面，从而跳出局部极值，增强了泛化能力

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203282210469.png" alt="在这里插入图片描述" style="zoom:50%;" />

设置一个Tensor，其Size为[3,4,2,2]



### 1、BatchNorm

所有Norm方法无非都是减均值再除以标准差，无非是在哪个尺度上进行该操作的差异，而BatchNorm是在一个batch上，同一个通道上面进行Norm，那么有多少个通道就会计算多少个均值和标准差。

Mean=所有batch同一个通道上的均值

Std=所有batch同一个通道上的标准差

batchSize过小时数值计算不稳定，batchSize一般取16

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203282224799.png" alt="在这里插入图片描述" style="zoom:50%;" />



### 2、LayerNorm

LayerNorm可以在3种不同的尺度进行



方式一：

```python
nn.LayerNorm(normalized_shape=[4,2,2])
```

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203282227103.png" alt="在这里插入图片描述" style="zoom:50%;" />

方式二：

```python
nn.LayerNorm(normalized_shape=[2,2])
```

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203282229355.png" alt="在这里插入图片描述" style="zoom:50%;" />





方式三：

```python
nn.LayerNorm(normalized_shape=2)
```

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203282230241.png" alt="在这里插入图片描述" style="zoom:50%;" />



### 3、GroupNorm

适合小batch输入

```python
nn.GroupNorm(num_groups=2, num_channels=4)
```

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203282237772.png)









## 残差网络

### 概念

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203292243183.jpeg)

残差神经网络（ResNet）是一种人工神经网络（ANN），它建立在从大脑皮层的金字塔细胞中已知的构造上。残差神经网络是通过利用跳过连接，或捷径跳过一些层来实现的。典型的ResNet模型是用双层或三层跳过来实现的，这些跳过包含非线性（ReLU）和中间的批量归一化。





### 实现

```python
# 总的残差网络模型
class ResNet(nn.Module):
    def __init__(self,block,block_num,num_classes=1000,include_top=True):
        super(ResNet,self).__init__()
        
        self.include_top=include_top
        self.in_channel=64
        
        # 第一个卷积层，使用7*7的卷积核，步长为2，使数据维度减半
        self.conv1=nn.Conv2d(in_channels=3,out_channels=self.in_channel,
                             kernel_size=7,stride=2,padding=3,bias=False)
        # 将卷积后的数据进行标准化
        self.bn1=nn.BatchNorm2d(self.in_channel)
        self.relu=nn.ReLU(inplace=True)
        # 最大池化层，采用卷积核为3，同样也会是维度减半
        self.maxpool=nn.MaxPool2d(kernel_size=3, stride=2, padding=1)
        
        '''分别对应4个残差块，这里传参有个注意问题，
        除了layer1，其余传的都是stride=2，因为对应之前说的任何网络的2，3，4，层的卷积块的
        第一个层都要进行数据降维，而layer不用，只需要将卷积核数升高，
        而数据维度不用变
        64,128,256,512就对应每个残差块的最基层的卷积核数，
        50、101层网络每层最终输出是它的4倍'''
        self.layer1=self._make_layer(block,64,block_num[0])
        self.layer2=self._make_layer(block,128,block_num[1],stride=2)
        self.layer3=self._make_layer(block,256,block_num[2],stride=2)
        self.layer4=self._make_layer(block,512,block_num[3],stride=2)
        
        # 将所有卷积核中的数据进行池化(1,1)之后进行全连接
        if self.include_top:
            self.avgpool=nn.AdaptiveAvgPool2d((1,1))
            self.fc=nn.Linear(512*block.expansion,num_classes)
        
        for m in self.modules():
            if isinstance(m, nn.Conv2d):
                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
       
    # 创建每一个残差块网络，第一个参数对应使用BisicBlock或者是Bottleneck，
    # 第二个参数是每个残差网络最基层的卷积核层数，第三个是每个残差块有几个基本网络
    def _make_layer(self,block,channel,block_num,stride=1):
        downsample=None
        
        '''对应那条捷径，50、101的第一层的第一个卷积层需要捷径，
        以及任何网络的第2、3、4个残差网络的第一层需要捷径，
        且50、101第一层的捷径不需要降维，只需要将卷积核的个数升高，
        其它的都需要进行降维且增加卷积核'''
        if stride!=1 or self.in_channel!=channel*block.expansion:
            downsample=nn.Sequential(
                nn.Conv2d(self.in_channel,channel*block.expansion,
                          kernel_size=1,stride=stride,bias=False),
                nn.BatchNorm2d(channel*block.expansion))
        
        layers=[]
        
        # 添加第一个卷积层
        layers.append(block(self.in_channel,channel,stride=stride,downsample=downsample))
        
        self.in_channel=channel*block.expansion
        
        # 添加之后的层
        for i in range(1,block_num):
            layers.append(block(self.in_channel,channel))
        
        return nn.Sequential(*layers)
    
    def forward(self,x):
        out=self.conv1(x)
        out=self.bn1(out)
        out=self.relu(out)
        out=self.maxpool(out)
        
        out=self.layer1(out)
        out=self.layer2(out)
        out=self.layer3(out)
        out=self.layer4(out)
        
        if self.include_top:
            out=self.avgpool(out)
            out=torch.flatten(out,1)
            out=self.fc(out)
        
        return out

```







## 注意力机制

### 概念

注意力机制是深度学习常用的一个小技巧，它有多种多样的实现形式，尽管实现方式多样，但是每一种注意力机制的实现的核心都是类似的，就是注意力。

注意力机制的核心重点就是让网络关注到它更需要关注的地方。

当我们使用卷积神经网络去处理图片的时候，我们会更希望卷积神经网络去注意应该注意的地方，而不是什么都关注，我们不可能手动去调节需要注意的地方，这个时候，如何让卷积神经网络去自适应的注意重要的物体变得极为重要。

注意力机制就是实现网络自适应注意的一个方式。

一般而言，注意力机制可以分为通道注意力机制，空间注意力机制，以及二者的结合。





### SENet的实现

SENet是通道注意力机制的典型实现。

2017年提出的SENet是最后一届ImageNet竞赛的冠军，其实现示意图如下所示，对于输入进来的特征层，我们关注其每一个通道的权重，对于SENet而言，重点是获得输入进来的特征层，每一个通道的权值。利用SENet，我们可以让网络关注它最需要关注的通道。

其具体实现方式就是：

1. 对输入进来的特征层进行全局平均池化。
2. 然后进行两次全连接，第一次全连接神经元个数较少，第二次全连接神经元个数和输入特征层相同。
3. 在完成两次全连接后，我们再取一次Sigmoid将值固定到0-1之间，此时我们获得了输入特征层每一个通道的权值（0-1之间）。
4. 在获得这个权值后，我们将这个权值乘上原输入特征层即可。
   

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241230356.png)



```python
import torch
import torch.nn as nn
import math

class se_block(nn.Module):
    def __init__(self, channel, ratio=16):
        super(se_block, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Sequential(
                nn.Linear(channel, channel // ratio, bias=False),
                nn.ReLU(inplace=True),
                nn.Linear(channel // ratio, channel, bias=False),
                nn.Sigmoid()
        )

    def forward(self, x):
        b, c, _, _ = x.size()
        y = self.avg_pool(x).view(b, c)
        y = self.fc(y).view(b, c, 1, 1)
        return x * y

```



### CBAM的实现

CBAM将通道注意力机制和空间注意力机制进行一个结合，相比于SENet只关注通道的注意力机制可以取得更好的效果。其实现示意图如下所示，CBAM会对输入进来的特征层，分别进行通道注意力机制的处理和空间注意力机制的处理。

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241236767.png)



图像的上半部分为通道注意力机制，通道注意力机制的实现可以分为两个部分，我们会对输入进来的单个特征层，分别进行全局平均池化和全局最大池化。之后对平均池化和最大池化的结果，利用共享的全连接层进行处理，我们会对处理后的两个结果进行相加，然后取一个sigmoid，此时我们获得了输入特征层每一个通道的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。

图像的下半部分为空间注意力机制，我们会对输入进来的特征层，在每一个特征点的通道上取最大值和平均值。之后将这两个结果进行一个堆叠，利用一次通道数为1的卷积调整通道数，然后取一个sigmoid，此时我们获得了输入特征层每一个特征点的权值（0-1之间）。在获得这个权值后，我们将这个权值乘上原输入特征层即可。

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241240337.png)



```python
class ChannelAttention(nn.Module):
    def __init__(self, in_planes, ratio=8):
        super(ChannelAttention, self).__init__()
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.max_pool = nn.AdaptiveMaxPool2d(1)

        # 利用1x1卷积代替全连接
        self.fc1   = nn.Conv2d(in_planes, in_planes // ratio, 1, bias=False)
        self.relu1 = nn.ReLU()
        self.fc2   = nn.Conv2d(in_planes // ratio, in_planes, 1, bias=False)

        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))
        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))
        out = avg_out + max_out
        return self.sigmoid(out)

class SpatialAttention(nn.Module):
    def __init__(self, kernel_size=7):
        super(SpatialAttention, self).__init__()

        assert kernel_size in (3, 7), 'kernel size must be 3 or 7'
        padding = 3 if kernel_size == 7 else 1
        self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        avg_out = torch.mean(x, dim=1, keepdim=True)
        max_out, _ = torch.max(x, dim=1, keepdim=True)
        x = torch.cat([avg_out, max_out], dim=1)
        x = self.conv1(x)
        return self.sigmoid(x)

class cbam_block(nn.Module):
    def __init__(self, channel, ratio=8, kernel_size=7):
        super(cbam_block, self).__init__()
        self.channelattention = ChannelAttention(channel, ratio=ratio)
        self.spatialattention = SpatialAttention(kernel_size=kernel_size)

    def forward(self, x):
        x = x * self.channelattention(x)
        x = x * self.spatialattention(x)
        return x

```





### ECA的实现

ECANet是也是通道注意力机制的一种实现形式。ECANet可以看作是SENet的改进版。
ECANet的作者认为SENet对通道注意力机制的预测带来了副作用，捕获所有通道的依赖关系是低效并且是不必要的。
在ECANet的论文中，作者认为卷积具有良好的跨通道信息获取能力。

ECA模块的思想是非常简单的，它去除了原来SE模块中的全连接层，直接在全局平均池化之后的特征上通过一个1D卷积进行学习。

既然使用到了1D卷积，那么1D卷积的卷积核大小的选择就变得非常重要了，了解过卷积原理的同学很快就可以明白，1D卷积的卷积核大小会影响注意力机制每个权重的计算要考虑的通道数量。用更专业的名词就是跨通道交互的覆盖率。

如下图所示，左图是常规的SE模块，右图是ECA模块。ECA模块用1D卷积替换两次全连接。

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241242334.png)



```python
class eca_block(nn.Module):
    def __init__(self, channel, b=1, gamma=2):
        super(eca_block, self).__init__()
        kernel_size = int(abs((math.log(channel, 2) + b) / gamma))
        kernel_size = kernel_size if kernel_size % 2 else kernel_size + 1
        
        self.avg_pool = nn.AdaptiveAvgPool2d(1)
        self.conv = nn.Conv1d(1, 1, kernel_size=kernel_size, padding=(kernel_size - 1) // 2, bias=False) 
        self.sigmoid = nn.Sigmoid()

    def forward(self, x):
        y = self.avg_pool(x)
        y = self.conv(y.squeeze(-1).transpose(-1, -2)).transpose(-1, -2).unsqueeze(-1)
        y = self.sigmoid(y)
        return x * y.expand_as(x)

```





## 参数初始化

我们构建好网络，开始训练前，不能默认的将所有权重系数都初始化为零，因为所有卷积核的系数都相等时，提取特征就会一样，反向传播时的梯度也会存在对称性，网络会退化会线性模型。另外网络层数较深时，初始化权重过大，会出现梯度爆炸，而过小又会出现梯度消失。

一个好的权重可以获得可能更好的结果。这就是为什么迁移学习通常拿在 IMGAENET 训练好的模型，并可以获得高准确度的原因了，因为已经给定了一个非常好的初始化权重。

好的初始化应该满足以下两个条件：

- 让神经元各层激活值不会出现饱和现象；
- 各层激活值也不能为0。



pytorch对某一层初始化

```python
self.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3)
init.xavier_uniform(self.conv1.weight)
init.constant(self.conv1.bias, 0.1)
```

pytorch对整个网络初始化

```python
def weights_init(m):
    classname=m.__class__.__name__
    if classname.find('Conv') != -1:
        xavier(m.weight.data)
        xavier(m.bias.data)
net = Net()
net.apply(weights_init) #apply函数会递归地搜索网络内的所有module并把参数表示的函数应用到所有的module上。   
```

不建议访问以下划线为前缀的成员，他们是内部的，如果有改变不会通知用户。更推荐的一种方法是检查某个module是否是某种类型：

```python
def weights_init(m):
    if isinstance(m, nn.Conv2d):
        xavier(m.weight.data)
        xavier(m.bias.data)     
```







### 高斯初始化

采用高斯分布初始化权重参数

```python
import torch
import torch.nn as nn
w = torch.empty(3, 5)
nn.init.uniform_(w, a=0, b=1)      #初始化为（0, 1）范围内的均匀分布
nn.init.normal_(w, mean=0, std=1)  #初始化为均值为0， 标准差为1的正态分布
nn.init.constant_(w, 0.3)          # 全部初始化为常量值0.3
nn.init.eye_(w)                     # 初始化为单位矩阵(对角线为1)
print(w)
```







### Xavier初始化

首先有一个共识必须先提出：神经网络如果保持每层的信息流动是同一方差，那么会更加有利于优化。不然大家也不会去争先恐后地研究各种normalization方法。

不过，Xavier Glorot认为还不够，应该增强这个条件，好的初始化应该使得各层的激活值和梯度的方差在传播过程中保持一致，这个被称为Glorot条件。

如果反向传播每层梯度保持近似的方差，则信息能反馈到各层。而前向传播激活值方差近似相等，有利于平稳地学习。

当然为了做到这一点，对激活函数也必须作出一些约定。

(1) 激活函数是线性的，至少在0点附近，而且导数为1。

(2) 激活值关于0对称。

这两个都不适用于sigmoid函数和ReLU函数，而适合tanh函数。

```python
w = torch.empty(3, 3, 5, 5)          #fan_in=75, fan_out=75
nn.init.xavier_uniform_(w, gain=1)   #初始化为（-sqrt(6/150), sqrt(6/150)）范围内的均匀分布
nn.init.xavier_normal_(w, gain=1)    #初始化为mean=0, std=sqrt(2/150)的正态分布
```





### **Kaiming/He** 初始化

He初始化基本思想是，当使用ReLU做为激活函数时，Xavier的效果不好，原因在于，当RelU的输入小于0时，其输出为0，相当于该神经元被关闭了，影响了输出的分布模式。

因此He初始化，在Xavier的基础上，假设每层网络有一半的神经元被关闭，于是其分布的方差也会变小。经过验证发现当对初始化值缩小一半时效果最好，故He初始化可以认为是Xavier初始/2的结果。



```python
参数：
    tensor – n 维 torch.Tensor
    a – 该层后面一层的整流函数中负的斜率 (默认为 0，此时为 Relu)
    mode – ‘fan_in’ (default) 或者 ‘fan_out’。使用fan_in保持weights的方差在前向传播中不变；使用fan_out保持weights的方差在反向传播中不变。
    nonlinearity – 非线性函数 (nn.functional 中的名字)，推荐只使用 ‘relu’ 或 ‘leaky_relu’ (default)。

w = torch.empty(3, 3, 5, 5)          #fan_in=75, fan_out=75
nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')   #初始化为（-sqrt(6/75), sqrt(6/75)）范围内的均匀分布
nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')    #初始化为mean=0, std=sqrt(2/75)的正态分布
```





### 分层初始化

在实际项目，要根据网络中的卷积核，BN和Linear等进行分开初始化，代码如下：

```python
#coding:utf-8

import torch
import torch.nn as nn

class MyNet(nn.Module):
    
    def __init__(self):
        super(MyNet, self).__init__()
        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1)  
        self.bn1 = nn.BatchNorm2d(64, eps=1e-05, momentum=0.1)
        self.relu1 = nn.ReLU(inplace=True)
        self.maxpool = nn.MaxPool2d(2, stride=2, padding=0)
        
        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)   
        self.bn2 = nn.BatchNorm2d(128, eps=1e-05, momentum=0.1)
        self.relu2 = nn.ReLU(inplace=True)
        self.maxpool2 = nn.MaxPool2d(2, stride=2, padding=0)
        
        self.fc1 = nn.Linear(6 * 64 * 64, 150)
        self.relu3 = nn.ReLU(inplace=True)
        
        self.fc2 = nn.Linear(150, 3)
        self.softmax = nn.Softmax(dim=1)
        
    def froward(x):
        x = self.maxpool1(self.relu1(self.bn1(self.conv1(x))))
        x = self.maxpool2(self.relu2(self.bn2(self.conv2(x))))
        x = x.view(6 * 64 * 64, -1)
        x = self.relu3(self.fc1(x))
        x = self.softmax(self.fc1(x))
        return x

#初始化方法一        
def init_weights(net):
    for m in net.modules():
        if isinstance(m, nn.Conv2d):
            nn.init.kaiming_uniform_(m.weight, a=0, mode="fan_in", nonlinearity="relu")
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.BatchNorm2d):
            nn.init.constant_(m.weight, 1)
            nn.init.constant_(m.bias, 0)
        elif isinstance(m, nn.Linear):
            nn.init.normal_(m.weight, mean=0, std=1e-3)
            nn.init.constant_(m.bias, 0)
net = MyNet()
init_weights(net)
print(list(net.parameters())
    
#初始化方式二
def init_weights(m):
    if isinstance(m, nn.Conv2d):
        nn.init.xavier_uniform_(m.weight.data)
        nn.init.constant_(m.bias.data, 0.1)
    elif isinstance(m, nn.BatchNorm2d):
        m.weight.data.fill_(1)
        m.bias.data.zero_()
    elif isinstance(m, nn.Linear):
        m.weight.data.normal_(0, 0.01)
        m.bias.data.zero_()
net = MyNet()
net.apply(init_weights)
print(list(net.parameters()))


```









## 前馈神经网络



### 神经网络结构

（1）感知器

![preview](https://raw.githubusercontent.com/a1254898873/images/master/202203241151009.jpg)

感知器是所有神经网络中最基本的，也是更复杂的神经网络的基本组成部分。 它只连接一个输入神经元和一个输出神经元。



（2）两层神经网络(多层感知器MLP)

![NN](https://raw.githubusercontent.com/a1254898873/images/master/202203241151010.png)

单层神经网络无法解决异或问题。但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。不过两层神经网络的计算是一个问题，没有一个较好的解法。1986年，Rumelhar和Hinton等人提出了反向传播（Backpropagation，BP）算法，解决了两层神经网络所需要的复杂计算量问题，从而带动了业界使用两层神经网络研究的热潮。

两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。

与单层神经网络不同。理论证明，两层神经网络可以无限逼近任意连续函数。



（3）多层神经网络

![NN](https://raw.githubusercontent.com/a1254898873/images/master/202203241151012.jpg)

除了输入、输出两层，其它的叫隐藏层。每个神经元是一个决策单元，整个神经网络是一个复杂的决策网络。它要做的事，就是通过隐藏层，把输入层处理成我们想要的输出层。

多层神经网络中，输出也是按照一层一层的方式来计算。从最外面的层开始，算出所有单元的值以后，再继续计算更深一层。只有当前层所有单元的值都计算完毕以后，才会算下一层。有点像计算向前不断推进的感觉。所以这个过程叫做 “正向传播” 。



### 神经网络实现

1、准备数据

```python
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.model_selection import train_test_split
```

```python
X, y = datasets.make_moons(n_samples=1000, noise=0.2, random_state=100)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)
print(X.shape, y.shape)
```

2、网络层

```python
class Layer:
    # 全链接网络层
    def __init__(self, n_input, n_output, activation=None, weights=None, bias=None):
        """
        :param int n_input: 输入节点数 
        :param int n_output: 输出节点数         
        :param str activation: 激活函数类型         
        :param weights: 权值张量，默认类内部生成         
        :param bias: 偏置，默认类内部生成 
        """
        self.weights = weights if weights is not None else np.random.randn(n_input, n_output) * np.sqrt(1 / n_output) 
        self.bias = bias if bias is not None else np.random.rand(n_output) * 0.1
        self.activation = activation # 激活函数类型，如’sigmoid’         
        self.activation_output = None # 激活函数的输出值 o         
        self.error = None  # 用于计算当前层的 delta 变量的中间变量 
        self.delta = None  # 记录当前层的 delta 变量，用于计算梯度 
    
    def activate(self, X):
        # 前向计算函数
        r = np.dot(X, self.weights) + self.bias # X@W + b
        # 通过激活函数，得到全连接层的输出 o (activation_output)      
        self.activation_output = self._apply_activation(r) 
        return self.activation_output
    
    def _apply_activation(self, r): # 计算激活函数的输出
        if self.activation is None:
            return r # 无激活函数，直接返回
        elif self.activation == 'relu':
            return np.maximum(r, 0)
        elif self.activation == 'tanh':
            return np.tanh(r)
        elif self.activation == 'sigmoid':
            return 1 / (1 + np.exp(-r))
        
        return r
    
    def apply_activation_derivative(self, r):
        # 计算激活函数的导数
        # 无激活函数， 导数为 1
        if self.activation is None:
            return np.ones_like(r)
        # ReLU 函数的导数
        elif self.activation == 'relu':             
            grad = np.array(r, copy=True)             
            grad[r > 0] = 1.             
            grad[r <= 0] = 0.             
            return grad
        # tanh 函数的导数实现         
        elif self.activation == 'tanh':             
            return 1 - r ** 2 
        # Sigmoid 函数的导数实现         
        elif self.activation == 'sigmoid': 
            return r * (1 - r)
        return r

```

3、网络模型

```python
class NeuralNetwork:
    def __init__(self):
        self._layers = [] # 网络层对象列表
    
    def add_layer(self, layer):
        self._layers.append(layer)
    
    def feed_forward(self, X):
        # 前向传播（求导）
        for layer in self._layers:
            X = layer.activate(X)
        return X
    
    def backpropagation(self, X, y, learning_rate):
        # 反向传播算法实现
        # 向前计算，得到最终输出值
        output = self.feed_forward(X)
        for i in reversed(range(len(self._layers))): # 反向循环
            layer = self._layers[i]
            if layer == self._layers[-1]: # 如果是输出层
                layer.error = y - output
                # 计算最后一层的 delta，参考输出层的梯度公式
                layer.delta = layer.error * layer.apply_activation_derivative(output)
            else: # 如果是隐藏层
                next_layer = self._layers[i + 1]
                layer.error = np.dot(next_layer.weights, next_layer.delta)
                layer.delta = layer.error*layer.apply_activation_derivative(layer.activation_output)
        
        # 循环更新权值
        for i in range(len(self._layers)):
            layer = self._layers[i]
            # o_i 为上一网络层的输出
            o_i = np.atleast_2d(X if i == 0 else self._layers[i - 1].activation_output)
            # 梯度下降算法，delta 是公式中的负数，故这里用加号 
            layer.weights += layer.delta * o_i.T * learning_rate 
    
    def train(self, X_train, X_test, y_train, y_test, learning_rate, max_epochs):
        # 网络训练函数
        # one-hot 编码
        y_onehot = np.zeros((y_train.shape[0], 2)) 
        y_onehot[np.arange(y_train.shape[0]), y_train] = 1
        mses = [] 
        for i in range(max_epochs):  # 训练 100 个 epoch             
            for j in range(len(X_train)):  # 一次训练一个样本                 
                self.backpropagation(X_train[j], y_onehot[j], learning_rate)             
                if i % 10 == 0: 
                    # 打印出 MSE Loss                 
                    mse = np.mean(np.square(y_onehot - self.feed_forward(X_train)))                 
                    mses.append(mse)                 
                    print('Epoch: #%s, MSE: %f, Accuracy: %.2f%%' % 
                          (i, float(mse), self.accuracy(self.predict(X_test), y_test.flatten()) * 100)) 

        return mses
    
    def accuracy(self, y_predict, y_test): # 计算准确度
        return np.sum(y_predict == y_test) / len(y_test)
    
    def predict(self, X_predict):
        y_predict = self.feed_forward(X_predict) # 此时的 y_predict 形状是 [600 * 2]，第二个维度表示两个输出的概率
        y_predict = np.argmax(y_predict, axis=1)
        return y_predict        

```



4、网络训练

```python
nn = NeuralNetwork() # 实例化网络类 
nn.add_layer(Layer(2, 25, 'sigmoid'))  # 隐藏层 1, 2=>25 
nn.add_layer(Layer(25, 50, 'sigmoid')) # 隐藏层 2, 25=>50 
nn.add_layer(Layer(50, 25, 'sigmoid')) # 隐藏层 3, 50=>25 
nn.add_layer(Layer(25, 2, 'sigmoid'))  # 输出层, 25=>2 
```

```python
nn.train(X_train, X_test, y_train, y_test, learning_rate=0.01, max_epochs=50)
```







### 调参技巧

1、优化器

机器学习训练的目的在于更新参数，优化目标函数，常见优化器有SGD，Adagrad，Adadelta，Adam，Adamax，Nadam。其中SGD和Adam优化器是最为常用的两种优化器，SGD根据每个batch的数据计算一次局部的估计，最小化代价函数。

学习速率决定了每次步进的大小，因此我们需要选择一个合适的学习速率进行调优。学习速率太大会导致不收敛，速率太小收敛速度慢。因此SGD通常训练时间更长，但是在好的初始化和学习率调度方案的情况下，结果更可靠。

Adam优化器结合了Adagrad善于处理稀疏梯度和RMSprop善于处理非平稳目标的优点，能够自动调整学习速率，收敛速度更快，在复杂网络中表现更优。



2、权重初始化

深度学习中权重初始化对模型收敛速度和模型质量有着重要的影响。参数在刚开始不能全都初始化为0，因为如果所有的参数都是0，那么所有神经元的输出都将是相同的，在反向传播时同一层内所有的神经元的行为也都一致。





3、正则化

为了防止过拟合，可通过加入l1、l2正则化。从公式可以看出，加入l1正则化的目的是为了加强权值的稀疏性，让更多值接近于零。而l2正则化则是为了减小每次权重的调整幅度，避免模型训练过程中出现较大抖动。





4、感受野

感受野决定了网络在某一层看到多大范围，一般说来最后一层一定至少要能看到最大的有意义的物体，更大的感受野通常是无害的。感受野必须缓慢增大，这样才能建模不同距离下的空间相关性。

在达到相同感受野的情况下，多层小卷积核的性能一定比大卷积核更好，因为多层小卷积核的非线性更强，而且更有利于特征共享。



5、卷积核

无脑用3x3，filter数量2^n，第一层的filter, 数量不要太少. 否则根本学不出来(底层特征很重要)





6、批标准化

```python
m=nn.BatchNorm2d(2,affine=True) #affine参数设为True表示weight和bias将被使用
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151013.jpg)



七、初始化

无脑用xavier

（1）Xavier均匀分布

```python
torch.nn.init.xavier_uniform_(tensor, gain=1)
```

（2） Xavier正态分布

```python
torch.nn.init.xavier_normal_(tensor, gain=1)
```













## 卷积神经网络

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151014.jpg)



### 1.卷积层

相当于滤镜，将图片进行分块，对每一块进行特征处理，从而提取特征。

如果用图像处理上的专业术语，我们可以将卷积叫做锐化。卷积其实是想要强调某些特征，然后将特征强化后提取出来，不同卷积核关注图片上不同的特征。

在CNN中，我们就是通过不断的改变卷积核矩阵的值来关注不同的细节，提取不同的特征。也就是说，在我们初始化卷积核的矩阵值(即权重参数)后，我们通过梯度下降不断降低loss来获得最好的权重参数，整个过程都是自动调整的。

| 卷积层次 | 特征类型 |
| :------: | :------: |
| 浅层卷积 | 边缘特征 |
| 中层卷积 | 局部特征 |
| 深层卷积 | 全局特征 |

#### 1.1卷积基本属性

- 卷积核（Kernel）：卷积操作的感受野，直观理解就是一个滤波矩阵，普遍使用的卷积核大小为3×3、5×5等；
- 步长（Stride）：卷积核遍历特征图时每步移动的像素，如步长为1则每次移动1个像素，步长为2则每次移动2个像素（即跳过1个像素），以此类推；
- 填充（Padding）：处理特征图边界的方式，一般有两种，一种是对边界外完全不填充，只对输入像素执行卷积操作，这样会使输出特征图的尺寸小于输入特征图尺寸；另一种是对边界外进行填充（一般填充为0），再执行卷积操作，这样可使输出特征图的尺寸与输入特征图的尺寸一致；
- 通道（Channel）：卷积层的通道数（层数）。



例：

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151015.jpg)

我们选择一张6*6*1（1为通道数，这里是1）的图像，让它和3*3的卷积核进行卷积操作，最后得出4*4*1的一张图像。



#### 1.2 卷积的计算过程

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151016.jpg)

卷积的计算过程非常简单，当卷积核在输入图像上扫描时，将卷积核与输入图像中对应位置的数值逐个相乘，最后汇总求和，就得到该位置的卷积结果。不断移动卷积核，就可算出各个位置的卷积结果。

卷积后所得feature map尺寸大小计算公式如下：

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203271042972.gif)

像素宽度：W（Width）

填充大小：P（Padding）

卷积核大小：K（Kernel-size）

步长大小：S（stride）











#### 1.3 权值共享和局部连接

左边是全连接，右边是局部连接

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151017.jpg)

对于一个1000 × 1000的输入图像而言，如果下一个隐藏层的神经元数目为10^6个，采用全连接则有1000 × 1000 × 10^6 = 10^12个权值参数，如此数目巨大的参数几乎难以训练；而采用局部连接，隐藏层的每个神经元仅与图像中10 × 10的局部图像相连接，那么此时的权值参数数量为10 × 10 × 10^6 = 10^8，将直接减少4个数量级。

尽管减少了几个数量级，但参数数量依然较多。能不能再进一步减少呢？能！方法就是权值共享。具体做法是，在局部连接中隐藏层的每一个神经元连接的是一个10 × 10的局部图像，因此有10 × 10个权值参数，将这10 × 10个权值参数共享给剩下的神经元，也就是说隐藏层中10^6个神经元的权值参数相同，那么此时不管隐藏层神经元的数目是多少，需要训练的参数就是这 10 × 10个权值参数（也就是卷积核(也称滤波器)的大小），如下图。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151018.jpg)



这大概就是CNN的一个神奇之处，尽管只有这么少的参数，依旧有出色的性能。但是，这样仅提取了图像的一种特征，如果要多提取出一些特征，可以增加多个卷积核，不同的卷积核能够得到图像的不同映射下的特征，称之为Feature Map。如果有100个卷积核，最终的权值参数也仅为100 × 100 = 10^4个而已。另外，偏置参数也是共享的，同一种滤波器共享一个。





### 2.池化层

通过对提取的高维特征进行降维。

将一个区域的的信息压缩成一个值，完成信息的抽象，获得一定程度的平移旋转不变性。

池化操作就是用一个kernel，比如3x3的，就去输入图像上对应3x3的位置上，选取这九个数字中最大的作为输出结果。这就叫最大池化，能更好地保留纹理特征。

如果选取这九个数字中的平均值作为输出结果。这就叫均值池化，能保留数据整体特征。







### 3.全连接层

他的输入是前面卷积池化得到的结果，把结果“展平”，就是把得到的结果矩阵，平铺为一个列向量。

权重的维度是 期望输入*期望输出 的二维矩阵，偏置项是 1*期望输出 的行向量。





### 4、反向传播

#### 4.1 池化层的反向传播

​		池化层没有激活函数可以直接看成用线性激活函数

​		在前向传播算法时，池化层一般我们会用MAX或者Average对输入进行池化，池化的区域大小已知。现在我们反过来，$$\delta^l$$要从缩小后的误差，还原前一次较大区域对应的误差。

　　在反向传播时，我们首先会把$$\delta^l$$的所有子矩阵矩阵大小还原成池化之前的大小，然后如果是MAX，则把$$\delta^l$$的所有子矩阵的各个池化局域的值放在之前做前向传播算法得到最大值的位置。如果是Average，则把δl的所有子矩阵的各个池化局域的值取平均后放在还原后的子矩阵位置。这个过程一般叫做upsample。



<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203271610989.png" alt="image-20220327160955847" style="zoom:67%;" />





#### 4.2 卷积层的反向传播

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203271631119.jpeg)

卷积核可以理解为参数w，原图的delta误差，等于卷积结果的delta误差经过零填充后，与卷积核旋转180度后的卷积。





![equation](https://raw.githubusercontent.com/a1254898873/images/master/202203271628966.svg)









### 5、常见网络结构

- 分类
  - AlexNet
  - ZFNet
  - GoogleNet
  - SqueezeNet
  - VGG
- 物体检测
  - Fast-rcnn, Faster-rcnn
  - [YOLO: Real Time Object Detection](https://github.com/pjreddie/darknet/wiki/YOLO:-Real-Time-Object-Detection)
  - [SSD: Single Shot MultiBox Detector](https://github.com/weiliu89/caffe/tree/ssd)
- 人脸识别
  - DeepID
  - [Center Loss](https://github.com/ydwen/caffe-face)











## 循环神经网络

### RNN 网络

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151019.png)

公式： $$ s^{(t)}=f(s^{(t-1)};\theta) $$

1、中间层的输出作为输入和下一个样本数据一起作为输入，也叫循环层
2、具有记忆样本之间相关联系的能力



### LSTM

LSTM 网络，即为长短期记忆网络。因为该网络的结构，该网络适合处理序列中间隔和延时较长的事件。

在 LSTM 网络中，通过将细胞体结构复杂化，在算法中加入了判断信息是否有用的处理器。在网络中加入了三道 “ 门 ” ，分别叫做：遗忘门、输入门、输出门。信息进入网络中，网络会根据一定的规则来判断信息是否有用，有用的信息将加以保留，无用的信息将进行遗忘。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203301031491.png)









## 生成对抗网络

<img src="https://raw.githubusercontent.com/a1254898873/images/master/202203241151021.png" alt="640?wx_fmt=jpeg" style="zoom:50%;" />



### 损失函数

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203290957497.png)

D和G表示Generator和Discriminator网络，可理解为两个函数映射，D用来判别数据真伪，即得到数据x为真实的概率D(x)，G由噪声z生成数据G(z)。V是Value，可理解为真实样本与生成样本间的Variation。

minmax指：先固定G，调整D的参数使其能最大化地判别样本是真实的还是生成的；然后固定D，调整G的参数以最小化 生成样本与真实样本的差异，如此反复，然后D&G就亦敌亦友般手牵手茁壮成长了起来~G最终就可以生成非常逼真的数据啦









### 训练步骤

在 GAN 的架构中，生成器和鉴别器都需要训练。我们不希望先用所有的训练数据训练其中任何一方，再训练另一方。我们希望它们能一起学习，任何一方都不应该超过另一方太多。

下面的三步训练循环是实现这一目标的一种方法。

- 第 1 步——向鉴别器展示一个真实的数据样本，告诉它该样本的分类应该是 1.0。
- 第 2 步——向鉴别器显示一个生成器的输出，告诉它该样本的分类应该是 0.0。
- 第 3 步——向鉴别器显示一个生成器的输出，告诉生成器结果应该是 1.0。



![图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203282256696.jpeg)

第 1 步最简单，我们也最熟悉。我们向鉴别器展示一幅实际数据集中的图像，并让它对图像进行分类。输出应为 1.0，我们再用损失来更新鉴别器。

![图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203282256782.jpeg)



第 2 步同样是训练鉴别器，不过这一次我们向它展示的是生成器的图像。输出的结果应该是 0.0。我们只用损失来更新鉴别器。在这一步中，我们必须注意不要更新生成器。因为我们不希望它因为被鉴别器识破而受到奖励。稍后，在编写 GAN 的代码时，我们将看到具体如何防止更新通过计算图回到生成器。

![图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203282257279.jpeg)

第 3 步是训练生成器。我们先用它生成一个图像，并将生成的图像输入给鉴别器进行分类。鉴别器的预期输出应该是 1.0。换句话说，我们希望生成器能成功骗过鉴别器，让它误以为图像是真实的，而不是生成的。我们只用结果的损失来更新生成器，而不更新鉴别器。因为我们不希望因为错误分类而奖励鉴别器。在编码时，这也很容易做到。





### 实现

```python
class Discriminator(nn.Module):
    def __init__(self):
        super(Discriminator, self).__init__()
        self.D = nn.Sequential(nn.Linear(28*28, 1),
                               nn.Sigmoid())  # [0,1] for binary cls
    def forward(self, x):
        return self.D(x)

class Generator(nn.Module):
    def __init__(self):
        super(Generator, self).__init__()
        self.G = nn.Sequential(nn.Linear(100, 28*28),
                               nn.Tanh())  # [-1,1]
    def forward(self, x):
        return self.G(x)

D = Discriminator()
G = Generator()

loss_fn = nn.BCELoss() # binary cross entropy
D_optim = torch.optim.Adam(D.parameters())
G_optim = torch.optim.Adam(G.parameters())
```



上面是模型、损失函数、优化器的定义，其重点是：D最终要判别一张图是否real，因此最终要映射到1维，并将数值范围压缩到[0,1]；G要生成一张图，因此要将随机生成的noise最终映射到28*28维，并将数值范围压缩到[-1,1]。

GAN的核心也就是下面的训练部分，即分别计算D与G的loss并反向传播更新参数（G生成前图像前，先随机生成指定维度的高斯分布）：

```python
# train D: to discriminate real or fake, 2 losses
real_img = real_img.view(batch_size, -1)  # [b,28,28]->[b,28*28]
real_img_score = D(real_img)
z = torch.randn(batch_size, 100)
fake_img = G(z)
fake_img_score = D(fake_img)

real_img_label = torch.ones(batch_size)
fake_img_label = torch.zeros(batch_size)
D_loss = loss_fn(real_img_score, real_img_label) + loss_fn(fake_img_score, fake_img_label)
D_loss.backward()
D_optim.step()

# train G after training D: to generate more real img -> "Adversarial"
G.zero_grad()
new_score = D(fake_img)
G_loss = loss_fn(new_score, real_img_label)
G_loss.backward()
G_optim.step()
```









## Pytorch

### 1.张量

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151022.png)

张量是PyTorch最基本的操作对象，英文名称为Tensor，它表示的是一个多维的矩阵。比如零维是一个点，一维就是向量，二维就是一般的矩阵，多维就相当于一个多维的数组，这和numpy是对应的，而且 Pytorch 的 Tensor 可以和 numpy 的ndarray相互转换，唯一不同的是Pytorch可以在GPU上运行，而numpy的 ndarray 只能在CPU上运行。

#### 1.1 创建tensor矩阵

```python
a = torch.Tensor([[2,3], [4,8], [7, 9]])
```

torch.Tensor默认的是torch.FloatTensor数据类型



#### 1.2 tensor矩阵与ndarray相互转换

```python
e = np.array([[2, 3],[4, 5]])

torch_e = torch.from_numpy(e)

f_torch_e = torch_e.float()
```

ndarray转tensor后类型是int32.如果需要转换成float需要使用torch_e.float()





#### 1.3  函数normal

```python
normal(mean, std, *, generator=None, out=None)
```

该函数返回从单独的正态分布中提取的随机数的张量，该正态分布的均值是mean，标准差是std。

```python
torch.normal(mean=0.,std=1.,size=(2,2))
```

一个标准正态分布N～(0,1)，提取一个2x2的矩阵





#### 1.4  张量的乘法

（1）torch.mul(a, b)是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵；

（2）torch.mm(a, b)是矩阵a和b矩阵相乘，比如a的维度是(1, 2)，b的维度是(2, 3)，返回的就是(1, 3)的矩阵。





#### 1.5 用Tensor建立简单的神经网络

（1）Tensor提供了最基本的数值运算，但神经网络运算就是张量计算，因此可以直接利用Tensor来搭建神经网络

（2）但由于单纯的Tensor是不具备自动求导的，因此，反向传播需要自己根据梯度公式来实现



```python
from sklearn.datasets import load_boston
from sklearn import preprocessing

dtype = torch.FloatTensor
# dtype = torch.cuda.FloatTensor

# 载入数据，并预处理
X, y = load_boston(return_X_y=True)
X = preprocessing.scale(X[:100,:])
y = preprocessing.scale(y[:100].reshape(-1, 1))

# 定义超参数
data_size, D_input, D_output, D_hidden = X.shape[0], X.shape[1], 1, 50
lr = 1e-5
epoch = 200000

# 转换为Tensor
# X = torch.Tensor(X).type(dtype)
# y = torch.Tensor(y).type(dtype)
X = torch.from_numpy(X).type(dtype)
y = torch.from_numpy(y).type(dtype)

# 定义训练参数
w1 = torch.randn(D_input, D_hidden).type(dtype)
w2 = torch.randn(D_hidden, D_output).type(dtype)

# 进行训练
for i in range(epoch):
    
    # 前向传播
    h = torch.mm(X, w1)                # 计算隐层
    h_relu = h.clamp(min=0)            # relu
    y_pred = torch.mm(h_relu, w2)      # 输出层
    
    # loss计算，使用L2损失函数
    loss = (y_pred - y).pow(2).sum()
    
    if i % 10000 == 0:
        print('epoch: {} loss: {}'.format(i, loss))
    
    # 反向传播，计算梯度
    grad_y_pred = 2.0 * (y_pred - y)
    grad_w2 = h_relu.t().mm(grad_y_pred)
    grad_h_relu = grad_y_pred.mm(w2.t())
    grad_h = grad_h_relu.clone()
    grad_h[h < 0] = 0
    grad_w1 = X.t().mm(grad_h)

    # 更新计算的梯度
    w1 -= lr * grad_w1
    w2 -= lr * grad_w2
```











### 2.Variable

它是一个可以变化的变量，符合反向传播和参数更新的属性，而tensor不能反向传播

pytorch中的tensor就像鸡蛋，Variable就像装鸡蛋的篮子

Variable 有三个比较重要的组成属性：data、grad和grad_fn。通过data可以取出 Variable 里面的tensor数值，grad_fn表示的是得到这个Variable的操作。比如通过加减还是乘除来得到的，最后grad是这个Variable的反向传播梯度，如下所示：

```python
import torch
from torch.autograd import Variable

# Create Variable

x = Variable(torch.Tensor([1]), requires_grad=True)
w = Variable(torch.Tensor([2]), requires_grad=True)
b = Variable(torch.Tensor([3]), requires_grad=True)

# Build a computational graph.
y = w * x + b    # y = 2 * x + b

# Compute gradients
y.backward()  #same as y.backward(torch.FloatTensor([1]))
# Print out the gradients.
print(x.grad)    # x.grad = 2
print(w.grad)    # x.grad = 1
print(b.grad)    # x.grad = 1
```

构建Variable，要注意得传入一个参数requires_grad=True，这个参数表示是否对这个变量求梯度，默认的是False，也就是不对这个变量求梯度，这里我们希望得到这些变量的梯度，所以需要传入这个参数。

从上面的代码中，我们注意到了一行y.backward()，这一行代码就是所谓的自动求导，这其实等价于y.backward(torch.FloatTensor([1]))，只不过对于标量求导里面的参数就可以不写了，自动求导不需要你再去明确地写明哪个函数对哪个函数求导，直接通过这行代码就能对所有的需要梯度的变量进行求导，得到它们的梯度，然后通过x.grad可以得到x的梯度。

上面是标量的求导，同时也可以做矩阵求导，比如：

```python
import torch
from torch.autograd import Variable

x = torch.randn(3)
x = Variable(x, requires_grad=True)

y = x * 3
print(y)

y.backward(torch.FloatTensor([1, 0.1, 0.01]))
print(x.grad)
```

相当于给出了一个三维向量去做运算，这时候得到的结果y就是一个向量，这里对这个向量求导就不能直接写成y.backward()，这样程序就会报错的。这个时候需要传入参数声明，比如y.backward(troch.FloatTensor([1, 1, 1]))，这样得到的结果就是它们每个分量的梯度，或者可以传入y.backward(torch.FloatTensor([1, 0.1, 0.01]))，这样得到的梯度就是它们原本的梯度分别乘上1, 0.1 和 0.01。





### 3.神经网络流程

#### 3.1 构造函数

构造合适的数据集，训练集（train）与测试集（test）。

数据读取可以使用torch的数据读取工具torch.utils.data.DataLoader。

```python
train_iter = torch.utils.data.DataLoader(mnist_train, batch_size=batch_size, shuffle=True, num_workers=num_workers)
test_iter = torch.utils.data.DataLoader(mnist_test, batch_size=batch_size, shuffle=False, num_workers=num_workers)
```



#### 3.2  定义模型结构

```python
class Net(torch.nn.Module):
    def __init__(self, n_feature, n_hidden, n_output):
        super(Net, self).__init__()
        self.hidden = torch.nn.Linear(n_feature, n_hidden)
        self.predict = torch.nn.Linear(n_hidden, n_output)

    def forward(self, x):
        x = F.relu(self.hidden(x))
        x = self.predict(x)
        return x

net1 = Net(1, 10, 1)   # 这是我们用这种方式搭建的 net1
```

```python
net2 = torch.nn.Sequential(
    torch.nn.Linear(1, 10),
    torch.nn.ReLU(),
    torch.nn.Linear(10, 1)
)
```







#### 3.3 定义损失函数

```python
# 分类问题
loss = torch.nn.CrossEntropyLoss()
# 回归问题
loss = torch.nn.MSELoss()
```

如无特殊需求，直接使用torch的损失函数即可。

CrossEntropyLoss()自带Softmax



#### 3.4 定义优化算法

```python
optimizer = torch.optim.SGD(net.parameters(), lr=learning_rate, weight_decay=0.01)
```

只有L2范数，没有L1范数

在定义优化器的时候设定weigth_decay，即L2范数前面的λ参数。



```python
# 初始化模型
net = MLP()
# 定义优化算法
optimizer = torch.optim.Adam(net.parameters())
```



#### 3.5 模型训练

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151023.png)

```python
# optimizer 是训练的工具
optimizer = torch.optim.SGD(net.parameters(), lr=0.2)  # 传入 net 的所有参数, 学习率
loss_func = torch.nn.MSELoss()      # 预测值和真实值的误差计算公式 (均方差)

for t in range(100):
    prediction = net(x)     # 喂给 net 训练数据 x, 输出预测值

    loss = loss_func(prediction, y)     # 计算两者的误差

    optimizer.zero_grad()   # 清空上一步的残余更新参数值
    loss.backward()         # 误差反向传播, 计算参数更新值
    optimizer.step()  
```

为什么每一轮batch需要设置optimizer.zero_grad：

根据pytorch中的backward()函数的计算，当网络参量进行反馈时，梯度是被积累的而不是被替换掉；但是在每一个batch时毫无疑问并不需要将两个batch的梯度混合起来累积，因此这里就需要每个batch设置一遍zero_grad 了。



#### 3.6 保存和提取

```python
torch.save(net1, 'net.pkl')  # 保存整个网络
torch.save(net1.state_dict(), 'net_params.pkl')   # 只保存网络中的参数 (速度快, 占内存少)
```



提取整个神经网络

```python
def restore_net():
    # restore entire net1 to net2
    net2 = torch.load('net.pkl')
    prediction = net2(x)
```

只提取网络参数

```python
def restore_params():
    # 新建 net3
    net3 = torch.nn.Sequential(
        torch.nn.Linear(1, 10),
        torch.nn.ReLU(),
        torch.nn.Linear(10, 1)
    )

    # 将保存的参数复制到 net3
    net3.load_state_dict(torch.load('net_params.pkl'))
    prediction = net3(x)
```





### 4.损失函数

#### 4.1 L1Loss

绝对值误差, 主要应用在回归的任务中。

```python
import torch
from torch import nn
input_data = torch.FloatTensor([[3], [4], [5]])   # batch_size, output
target_data = torch.FloatTensor([[2], [5], [8]])   # batch_size, output 
loss_func = nn.L1Loss()
loss = loss_func(input_data, target_data)
print(loss)  # 1.6667
```

验证代码:

```python
print((abs(3-2) + abs(4-5) + abs(5-8)) / 3) 
```



#### 4.2 MSELoss(L2Loss)

均方误差, 主要应用在回归的任务中

```python
import torch
from torch import nn
input_data = torch.FloatTensor([[3], [4], [5]])   # batch_size, output
target_data = torch.FloatTensor([[2], [5], [8]])   # batch_size, output 
loss_func = nn.MSELoss()
loss = loss_func(input_data, target_data)
print(loss)   # 3.6667
```

验证代码

```python
print(((3-2)**2 + (4-5)**2 + (5-8)**2)/3)
```



#### 4.3 NLLLoss

负对数似然损失，主要应用在分类任务中。它先通过logSoftmax()，然后把label对应的输出值拿出来，负号去掉，然后平均。

```python
# 三个样本 进行三分类 使用NLLLoss
import torch
from torch import nn

input = torch.randn(3, 3)
print(input)
# tensor([[ 0.0550, -0.5005, -0.4188],
#         [ 0.7060,  1.1139, -0.0016],
#         [ 0.3008, -0.9968,  0.5147]])
label = torch.LongTensor([0, 2, 1])   # 真实label
loss_func = nn.NLLLoss()
loss = loss_func(temp, label)
print(loss)   # 损失1.6035
```

验证代码:

```python
output = torch.FloatTensor([
        [ 0.0550, -0.5005, -0.4188],
        [ 0.7060,  1.1139, -0.0016],
        [ 0.3008, -0.9968,  0.5147]]
        )

# 1. softmax + log = torch.log_softmax()
sm = nn.Softmax(dim=1)
temp = torch.log(sm(input))
print(temp)  
# tensor([[-0.7868, -1.3423, -1.2607],
#         [-1.0974, -0.6896, -1.8051],
#         [-0.9210, -2.2185, -0.7070]])
# 2. 因为label为[0, 2, 1]  
#    因此第一行取第一个值-0.7868。第二行取第三个值-1.8051，第三行取第二个值-2.2185。然后把负号直接扔掉。 说白的 就是去对数的负值成对应的label  也就是交叉熵。
print((0.7868 + 1.8051 + 2.2185) / 3)   # 输出1.6034666666666666
```



#### 4.4 CrossEntropyLoss

交叉熵，实际上它是由nn.LogSoftmax()和nn.NLLLoss()组成。 主要应用在多分类的问题中(二分类也可以用)

```python
# 三个样本进行三分类 和上面的数据一样
import torch
from torch import nn
loss_func1 = nn.CrossEntropyLoss()
output = torch.FloatTensor([
        [ 0.0550, -0.5005, -0.4188],
        [ 0.7060,  1.1139, -0.0016],
        [ 0.3008, -0.9968,  0.5147]]
        )

true_label = torch.LongTensor([0, 2, 1])   # 注意这里的label id必须从0开始 不能说label id是1，2，3 必须是0，1，2
loss = loss_func1(output, true_label)
print(loss)  # 输出: 1.6035
```





### 5.优化器

#### 5.1 构造一个优化器

要构造一个Optimizer，你必须给它一个包含参数（必须都是Variable对象）进行优化。然后，您可以指定optimizer的参 数选项，比如学习率，权重衰减等。

```python
# SGD 就是随机梯度下降
opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)
# momentum 动量加速,在SGD函数里指定momentum的值即可
opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=0.8)
# RMSprop 指定参数alpha
opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=0.9)
# Adam 参数betas=(0.9, 0.99)
opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(0.9, 0.99))
```

#### 5.2 单次优化

```python
for input, target in dataset:
    optimizer.zero_grad()
    output = model(input)
    loss = loss_fn(output, target)
    loss.backward()
    optimizer.step()
```

这是大多数optimizer所支持的简化版本。一旦梯度被如backward()之类的函数计算好后，我们就可以调用该函数。





### 6.激活函数

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151024.png)

激活函数是神经网络中对输入数据转换的方法，通过激活函数后将输入值转化为其他信息；在神经网络的隐藏层中，激活函数负责将进入神经元的信息汇总转换为新的输出信号，传递给下一个神经元。

如果不使用激活函数，每个输入节点的输入都是一样的，成为了原始的感知机，没有信号的转换，使得网络的逼近能力有限，无法充分发挥网络的强大学习能力。

性质：

- 非线性： 当激活函数是线性的时候，一个两层的神经网络就可以逼近基本上所有的函数了。但是，如果激活函数是恒等激活函数的时候（即f(x)=x），就不满足这个性质了，而且如果MLP使用的是恒等激活函数，那么其实整个网络跟单层神经网络是等价的。
- 可微性： 当优化方法是基于梯度的时候，这个性质是必须的。
- 单调性： 当激活函数是单调的时候，单层网络能够保证是凸函数。
- f(x)≈x： 当激活函数满足这个性质的时候，如果参数的初始化是random的很小的值，那么神经网络的训练将会很高效；如果不满足这个性质，那么就需要很用心的去设置初始值。
- 输出值的范围： 当激活函数输出值是 有限 的时候，基于梯度的优化方法会更加 稳定，因为特征的表示受有限权值的影响更显著；当激活函数的输出是 无限 的时候，模型的训练会更加高效，不过在这种情况小，一般需要更小的learning rate





#### 6.1 sigmod函数

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151025.png)

 sigmod激励函数符合实际，当输入很小时，输出接近于0；当输入很大时，输出值接近1，但是sigmoid函数作为非线性激活函数，但是其并不被经常使用。会出现梯度消失问题。

sigmoid函数可用在网络最后一层，作为输出层进行二分类，尽量不要使用在隐藏层。　



#### 6.2 tanh函数 

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151026.png)

会出现梯度消失问题





#### 6.3 Relu函数

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151027.png)

其弥补了sigmoid函数以及tanh函数的梯度消失问题。

 ReLU目前仍是最常用的activation function，在搭建人工神经网络的时候推荐优先尝试！







### 7.常用normalization函数

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203282116966.png)

归一化层，目前主要有这几个方法，Batch Normalization（2015年）、Layer Normalization（2016年）、Instance Normalization（2017年）、Group Normalization（2018年）、Switchable Normalization（2019年）；





#### 7.1 batchNorm

在卷积神经网络的卷积层之后总会添加BatchNorm2d进行数据的归一化处理，这使得数据在进行Relu之前不会因为数据过大而导致网络性能的不稳定

batchNorm是在batch上，对NHW做归一化，对小batchsize效果不好



```python
class torch.nn.BatchNorm2d(num_features, eps=1e-05, momentum=0.1, affine=True)
```





#### 7.2 LayerNorm

layerNorm在通道方向上，对CHW归一化，主要对RNN作用明显

```python
CLASS torch.nn.LayerNorm(normalized_shape, eps=1e-05, elementwise_affine=True)
```





#### 7.3 InstanceNorm2d

instanceNorm在图像像素上，对HW做归一化，用在风格化迁移

```python
CLASS torch.nn.InstanceNorm2d(num_features, eps=1e-05, momentum=0.1, affine=False, track_running_stats=False)
```





#### 7.4 GroupNorm

GroupNorm将channel分组，然后再做归一化

```python
CLASS torch.nn.GroupNorm(num_groups, num_channels, eps=1e-05, affine=True)
```











### 8.卷积神经网络

#### 8.1 读取数据

在学习Pytorch的教程时，加载数据许多时候都是直接调用torchvision.datasets里面集成的数据集，直接在线下载，然使用torch.utils.data.DataLoader进行加载。

那么，我们怎么使用我们自己的数据集，然后用DataLoader进行加载呢？

常见的两种形式的导入：

1. 一种是整个数据集都在一个文件下，内部再另附一个label文件，说明每个文件的状态。这种存放数据的方式可能更时候在非分类问题上得到应用。
2. 一种则是更适合在分类问题上，即把不同种类的数据分为不同的文件夹存放起来。这样，我们可以从文件夹或文件名得到label。



（1）Dataset

这种方法是官方导航介绍的。
torch.utils.data.Dataset是一个抽象类，用户想要加载自定义的数据只需要继承这个类，并且覆写其中的两个方法即可：

1. len:实现len(dataset)返回整个数据集的大小。
2. getitem用来获取一些索引的数据，使dataset[i]返回数据集中第i个样本。
3. 不覆写这两个方法会直接返回错误。
   

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151028.jpg)

```python
# coding: utf-8
from PIL import Image
from torch.utils.data import Dataset
class MyDataset(Dataset):
def __init__(self, txt_path, transform = None, target_transform = None):
    fh = open(txt_path, 'r')
    imgs = []
    for line in fh:
        line = line.rstrip()
        words = line.split()
        imgs.append((words[0], int(words[1])))
        self.imgs = imgs 
        self.transform = transform
        self.target_transform = target_transform
def __getitem__(self, index):
    fn, label = self.imgs[index]
    img = Image.open(fn).convert('RGB') 
    if self.transform is not None:
        img = self.transform(img) 
    return img, label
def __len__(self):
    return len(self.imgs)
```

首先看看初始化，初始化中从我们准备好的txt里获取图片的路径和标签，并且存储在self.imgs，self.imgs就是上面提到的list，其一个元素对应一个样本的路径和标签，其实就是txt中的一行。

初始化中还会初始化transform，transform是一个Compose类型，里边有一个list，list中就会定义了各种对图像进行处理的操作，可以设置减均值，除标准差，随机裁剪，旋转，翻转，仿射变换等操作。

在这里我们可以知道，一张图片读取进来之后，会经过数据处理（数据增强），最终变成输入模型的数据。这里就有一点需要注意，PyTorch的数据增强是将原始图片进行了处理，并不会生成新的一份图片，而是“覆盖”原图，当采用randomcrop之类的随机操作时，每个epoch输入进来的图片几乎不会是一模一样的，这达到了样本多样性的功能。

然后看看核心的 getitem函数：

第一行：self.imgs 是一个list，也就是一开始提到的list，self.imgs的一个元素是一个str，包含图片路径，图片标签，这些信息是从txt文件中读取

第二行：利用Image.open对图片进行读取，img类型为 Image ，mode=‘RGB’

第三行与第四行： 对图片进行处理，这个transform里边可以实现 减均值，除标准差，随机裁剪，旋转，翻转，放射变换，等等操作，这个放在后面会详细讲解。

当Mydataset构建好，剩下的操作就交给DataLoder，在DataLoder中，会触发Mydataset中的getiterm函数读取一张图片的数据和标签，并拼接成一个batch返回，作为模型真正的输入。下一小节将会通过一个小例子，介绍DataLoder是如何获取一个batch，以及一张图片是如何被PyTorch读取，最终变为模型的输入的。





（2）ImageFolder

pytorch几乎将上述所有工作都封装起来供我们使用，其中一个工具就是torchvision.datasets.ImageFolder,用于加载用户自定义的数据，要求我们的数据要有如下结构：将图片按类别分文件夹存放。

```python
ImageFolder(root, transform=None, target_transform=None, loader=default_loader)
```

- root：在root指定的路径下寻找图片
- transform：对PIL Image进行的转换操作，transform的输入是使用loader读取图片的返回对象
- target_transform：对label的转换
- loader：给定路径后如何读取图片，默认读取为RGB格式的PIL Image对象

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151029.png)

label是按照文件夹名顺序排序后存成字典，即{类名:类序号(从0开始)}，一般来说最好直接将文件夹命名为从0开始的数字，这样会和ImageFolder实际的label一致，如果不是这种命名规范，建议看看self.class_to_idx属性以了解label和文件夹名的映射关系。



```python
from torchvision import transforms as T
import matplotlib.pyplot as plt
from torchvision.datasets import ImageFolder


dataset = ImageFolder('data/dogcat_2/')

# cat文件夹的图片对应label 0，dog对应1
print(dataset.class_to_idx)

# 所有图片的路径和对应的label
print(dataset.imgs)

# 没有任何的transform，所以返回的还是PIL Image对象
#print(dataset[0][1])# 第一维是第几张图，第二维为1返回label
#print(dataset[0][0]) # 为0返回图片数据
plt.imshow(dataset[0][0])
plt.axis('off')
plt.show()

```

加上transform

```python
normalize = T.Normalize(mean=[0.4, 0.4, 0.4], std=[0.2, 0.2, 0.2])
transform  = T.Compose([
         T.RandomResizedCrop(224),
         T.RandomHorizontalFlip(),
         T.ToTensor(),
         normalize,
])
dataset = ImageFolder('data1/dogcat_2/', transform=transform)

# 深度学习中图片数据一般保存成CxHxW，即通道数x图片高x图片宽
#print(dataset[0][0].size())

to_img = T.ToPILImage()
# 0.2和0.4是标准差和均值的近似
a=to_img(dataset[0][0]*0.2+0.4)
plt.imshow(a)
plt.axis('off')
plt.show()

```





#### 8.2 transforms

采用transforms.Compose()，将一系列的transforms有序组合，实现时按照这些方法依次对图像操作。

```python
train_transform = transforms.Compose([
    transforms.Resize((32, 32)),  # 缩放
    transforms.RandomCrop(32, padding=4),  # 随机裁剪
    transforms.ToTensor(),  # 图片转张量，同时归一化0-255 ---》 0-1
    transforms.Normalize(norm_mean, norm_std),  # 标准化均值为0标准差为1
])

```

```python
# 构建MyDataset实例
train_data = RMBDataset(data_dir=train_dir, transform=train_transform)
# 构建DataLoder
train_loader = DataLoader(dataset=train_data, batch_size=BATCH_SIZE, shuffle=True)

```









#### 8.3 卷积

我们常用的卷积（Conv2d）在pytorch中对应的函数是：

```python

torch.nn.Conv2d(in_channels, 
                out_channels, 
                kernel_size, 
                stride=1, 
                padding=0, 
                dilation=1, 
                groups=1, 
                bias=True, 
                padding_mode='zeros')

```

in_channels参数代表输入特征矩阵的深度即channel，比如输入一张RGB彩色图像，那in_channels=3

out_channels参数代表卷积核的个数，使用n个卷积核输出的特征矩阵深度即channel就是n

kernel_size参数代表卷积核的尺寸，输入可以是int类型如3 代表卷积核的height=width=3，也可以是tuple类型如(3, 5)代表卷积核的height=3，width=5

stride参数代表卷积核的步距默认为1，和kernel_size一样输入可以是int类型，也可以是tuple类型

padding参数代表在输入特征矩阵四周补零的情况默认为0，同样输入可以为int型如1 代表上下方向各补一行0元素，左右方向各补一列0像素（即补一圈0），如果输入为tuple型如(2, 1) 代表在上方补两行下方补两行，左边补一列，右边补一列。



#### 8.4 模型

```python
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Sequential(  # input shape (1, 28, 28)
            nn.Conv2d(
                in_channels=1,      # input height
                out_channels=16,    # n_filters
                kernel_size=5,      # filter size
                stride=1,           # filter movement/step
                padding=2,      # 如果想要 con2d 出来的图片长宽没有变化, padding=(kernel_size-1)/2 当 stride=1
            ),      # output shape (16, 28, 28)
            nn.ReLU(),    # activation
            nn.MaxPool2d(kernel_size=2),    # 在 2x2 空间里向下采样, output shape (16, 14, 14)
        )
        self.conv2 = nn.Sequential(  # input shape (16, 14, 14)
            nn.Conv2d(16, 32, 5, 1, 2),  # output shape (32, 14, 14)
            nn.ReLU(),  # activation
            nn.MaxPool2d(2),  # output shape (32, 7, 7)
        )
        self.out = nn.Linear(32 * 7 * 7, 10)   # fully connected layer, output 10 classes

    def forward(self, x):
        x = self.conv1(x)
        x = self.conv2(x)
        x = x.view(x.size(0), -1)   # 展平多维的卷积图成 (batch_size, 32 * 7 * 7)
        output = self.out(x)
        return output

cnn = CNN()
```

这个 CNN 整体流程是 卷积(Conv2d) -> 激励函数(ReLU) -> 池化, 向下采样 (MaxPooling) -> 再来一遍 -> 展平多维的卷积成的特征图 -> 接入全连接层 (Linear) -> 输出

nn.Flatten(): 可以执行展平操作，返回一个一维数组

nn.Linear(): 用于设置网络中的全连接层的





### 9、循环神经网络

#### 9.1 LSTM模型构建方法

```python
net = nn.LSTM(input_size=3, hidden_size=10, num_layers=1, batch_first=True)
```

![image-20220211124113646](https://raw.githubusercontent.com/a1254898873/images/master/202203241151030.png)





#### 9.2 模型构建



```python
class RNN(nn.Module):
    def __init__(self):
        super(RNN, self).__init__()

        self.rnn = nn.LSTM(     # LSTM 效果要比 nn.RNN() 好多了
            input_size=28,      # 图片每行的数据像素点
            hidden_size=64,     # rnn hidden unit
            num_layers=1,       # 有几层 RNN layers
            batch_first=True,   # input & output 会是以 batch size 为第一维度的特征集 e.g. (batch, time_step, input_size)
        )

        self.out = nn.Linear(64, 10)    # 输出层

    def forward(self, x):
        # x shape (batch, time_step, input_size)
        # r_out shape (batch, time_step, output_size)
        # h_n shape (n_layers, batch, hidden_size)   LSTM 有两个 hidden states, h_n 是分线, h_c 是主线
        # h_c shape (n_layers, batch, hidden_size)
        r_out, (h_n, h_c) = self.rnn(x, None)   # None 表示 hidden state 会用全0的 state

        # 选取最后一个时间点的 r_out 输出
        # 这里 r_out[:, -1, :] 的值也是 h_n 的值
        out = self.out(r_out[:, -1, :])
        return out

rnn = RNN()
print(rnn)
"""
RNN (
  (rnn): LSTM(28, 64, batch_first=True)
  (out): Linear (64 -> 10)
)
"""
```







### 10、生成对抗网络

#### 10.1 准备数据

```python
BATCH_SIZE = 100

def mnist_data():
    transform = transforms.Compose([
        transforms.ToTensor(),
        transforms.Normalize((0.5,), (0.5,))
    ])
    return datasets.MNIST(root='./data', train=True, transform=transform, download=True)

data = mnist_data()

data_loader = torch.utils.data.DataLoader(data, batch_size=BATCH_SIZE, shuffle=True)

```





#### 10.2 判别器网络

​		可以想象，如果我们只给判别器输入生成器的生成图像，判别器会学会将所有图像不管是否真实都判定为Fake。这显然不是我们想要的结果，所以我们会在训练判别器时，分别给它一张真实的图像和一张假的图像（如上图所示），其需要找出哪个是真哪个是假，并以此计算损失。

  所以判别器的任务可以视为分类任务，其需要将输入图像分类为原始图像或生成图像，这是二元分类。

  我们搭建一个判别器网络，其共有四层，前三层每层都由一个线性层、激活函数LeakyReLU 和一个dropout 层组成，最后一层由一个线性层和Sigmoid 激活函数组成。

```python
class DiscriminatorNet(nn.Module):
    """
    A three hidden-layer discriminative neural network
    """
    def __init__(self):
        super().__init__()
        n_features = 784
        n_out = 1
        
        self.hidden0 = nn.Sequential(
            nn.Linear(n_features, 1024),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.hidden1 = nn.Sequential(
            nn.Linear(1024, 512),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.hidden2 = nn.Sequential(
            nn.Linear(512, 256),
            nn.LeakyReLU(0.2),
            nn.Dropout(0.3)
        )
        self.out = nn.Sequential(
            nn.Linear(256, n_out),
            nn.Sigmoid()
        )
        
    def forward(self, x):
        x = self.hidden0(x)
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.out(x)
        return x

```





#### 10.3 生成器网络

```python
class GeneratorNet(nn.Module):
    """
    A three hidden-layer generative neural network
    """
    def __init__(self):
        super().__init__()
        n_features = 100
        n_out = 784
        
        self.hidden0 = nn.Sequential(
            nn.Linear(n_features, 256),
            nn.LeakyReLU(0.2)
        )
        self.hidden1 = nn.Sequential(
            nn.Linear(256, 512),
            nn.LeakyReLU(0.2)
        )
        self.hidden2 = nn.Sequential(
            nn.Linear(512, 1024),
            nn.LeakyReLU(0.2)
        )
        self.out = nn.Sequential(
            nn.Linear(1024, n_out),
            nn.Tanh()
        )
        
    def forward(self, x):
        x = self.hidden0(x)
        x = self.hidden1(x)
        x = self.hidden2(x)
        x = self.out(x)
        return x
```





#### 10.4 模型训练

  我们需要同时训练两个模型，分别是判别器模型与生成器模型。其中判别器模型的训练过程中需要输入两幅图像，然后根据其判别结果分别计算损失，最后将损失相加再更新参数；而生成器模型的训练则较为简单，只需要根据判别器的判别结果进行参数更新即可。

  模型的训练过程可以遵循该顺序：1）梯度清零；2）获取输出；3）计算损失；4）反向传播；5）参数优化

  因为理想的判别器能够将真实图像标记为1，将生成图像标记为0。所以在分别对真实图像与生成图像的判别结果计算损失时，使其标签分别为同等尺寸的全1张量与全0张量。
		

```python
def train_discriminator(optimizer, loss_fn, input_real_data, input_fake_data):
    optimizer.zero_grad()
    
    # 对于真实样本的预测结果
    discriminated_real_data = discriminator(input_real_data)
    loss_real = loss_fn(discriminated_real_data, real_data_target(input_real_data.size(0)))
    loss_real.backward()
    
    # 对于生成样本的预测结果
    discriminated_fake_data = discriminator(input_fake_data)
    loss_fake = loss_fn(discriminated_fake_data, fake_data_target(input_fake_data.size(0)))
    loss_fake.backward()
    
    optimizer.step()
    
    return loss_real+loss_fake, discriminated_real_data, discriminated_fake_data

```

  理想的生成器能够生成使判别器判别为1（即认为是真实图像）的图像，因此在其计算损失时的标签为同等尺寸的全1张量。

```python
def train_generator(optimizer, loss_fn, input_fake_data):
    optimizer.zero_grad()
    output_discriminated = discriminator(input_fake_data)
    loss = loss_fn(output_discriminated, real_data_target(output_discriminated.size(0)))
    loss.backward()
    optimizer.step()
    return loss
```

```python
discriminator = DiscriminatorNet().to(device)
d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)

generator = GeneratorNet().to(device)
g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)

loss_fn = nn.BCELoss()

num_epochs = 100
num_test_samples = 3
```

  我们首先将真实样本与生成器的生成样本作为判别器的输入，对判别器进行训练。然后再通过判别器的预测结果对生成器进行训练。并间歇输出生成器的生成图像以观察模型训练结果。

```python
for epoch in range(num_epochs):
    for train_idx, (input_real_batch, _) in enumerate(data_loader):
        # training the discriminator
        input_real_data = images2vectors(input_real_batch).to(device)
        generated_fake_data = generator(noise(input_real_data.size(0))).detach()
        d_loss, discriminated_real, discriminated_fake = train_discriminator(d_optimizer, loss_fn, input_real_data, generated_fake_data)
        
        # training the generator
        generated_fake_data = generator(noise(input_real_batch.size(0)))
        g_loss = train_generator(g_optimizer, loss_fn, generated_fake_data)
        # print the info
        if train_idx % 10 == 0:
            print('Epoch:{} Loss:{} =={:.2f}%=='.format\
                 (epoch, g_loss, ((train_idx+epoch*num_batches)/(num_batches*num_epochs))*100))
            # output the images generated by generator
            if train_idx % 30 == 0:
                test_noise = noise(num_test_samples)
                generated = generator(test_noise).detach()
                discriminated = discriminator(generated)
                images = vectors2images(generated)

                fig = plt.figure()
                for i in range(3):
                    plt.subplot(1, 3, i+1)
                    plt.imshow(images[i][0], cmap='gray', interpolation='none')
                    plt.title('pred: {:.3f}'.format(discriminated[i].item()))
                    plt.xticks([])
                    plt.yticks([])
                    plt.show()
                    
print('End Training')

```







### 11、tensorboard

```python
import torch
import torchvision
from torchvision import datasets, transforms
# 可视化工具, SummaryWriter的作用就是，将数据以特定的格式存储到上面得到的那个日志文件夹中
from torch.utils.tensorboard import SummaryWriter

# 第一步：实例化对象。注：不写路径，则默认写入到 ./runs/ 目录
writer = SummaryWriter()

transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])
trainset = datasets.MNIST('mnist_train', train=True, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)
model = torchvision.models.resnet50(False)

# 让 ResNet 模型采用灰度而不是 RGB
model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)
images, labels = next(iter(trainloader))

grid = torchvision.utils.make_grid(images)

# 第二步：调用对象的方法，给文件夹存数据
writer.add_image('images', grid, 0)
writer.add_graph(model, images)
writer.close()

```















## TensorFlow

### 张量



基本操作

```python
# 定义张量常量
a = tf.constant(2)
b = tf.constant(3)
c = tf.constant(5)

# 各种张量操作
# 注意：张量也支持python的操作（ ，*,...）
add = tf.add(a,b)
sub = tf.subtract(a,b)
mul = tf.multiply(a,b)
div = tf.divide(a,b)
```



```python
# 矩阵乘法
matrix1 = tf.constant([[1,2],[3,4]])
matrix2 = tf.constant([[5,6],[7,8]])

product = tf.matmul(matrix1,matrix2)
# 展示张量
product
```



自动求导

```python
x=tf.Variable(initial_value=[[1.0,2.0,3.0],[4.0,5.0,6.0]])
 
with tf.GradientTape() as g:
    g.watch(x)
    with tf.GradientTape() as gg:
        gg.watch(x)
        y = x * x
    dy_dx = gg.gradient(y, x)     # 求一阶导数
d2y_dx2 = g.gradient(dy_dx, x)    # 求二阶导数

```

（1）创建一个GradientTape对象：g=tf.GradientTape()

（2）监视watch要求导的变量：g.watch(x)

（3）对函数进行求导：g.gredient(y,x)



### keras

#### 模型堆叠

```python
model = tf.keras.Sequential()
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
```

#### 网络配置

activation：设置层的激活函数。此参数由内置函数的名称指定，或指定为可调用对象。默认情况下，系统不会应用任何激活函数。

kernel_initializer 和 bias_initializer：创建层权重（核和偏差）的初始化方案。此参数是一个名称或可调用对象，默认为 "Glorot uniform" 初始化器。

kernel_regularizer 和 bias_regularizer：应用层权重（核和偏差）的正则化方案，例如 L1 或 L2 正则化。默认情况下，系统不会应用正则化函数。

```python
layers.Dense(32, activation='sigmoid')
layers.Dense(32, activation=tf.sigmoid)
layers.Dense(32, kernel_initializer='orthogonal')
layers.Dense(32, kernel_initializer=tf.keras.initializers.glorot_normal)
layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l2(0.01))
layers.Dense(32, kernel_regularizer=tf.keras.regularizers.l1(0.01))
```

#### 训练和评估

```python
model = tf.keras.Sequential()
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(32, activation='relu'))
model.add(layers.Dense(10, activation='softmax'))
model.compile(optimizer=tf.keras.optimizers.Adam(0.001),
             loss=tf.keras.losses.categorical_crossentropy,
             metrics=[tf.keras.metrics.categorical_accuracy])
```































## 手写数字识别

### 1、根据灰度值来识别

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151031.png)





### 2、转化为矩阵![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151032.png)







### 3、输入

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151033.png)



### 4、神经元

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151034.png)

每一个像素代表一个神经元， 一张28 * 28 的图片有784 个神经元， 神经元中的数字用灰度值表示， 白色代表1， 黑色代表0



### 5、识别

当我们识别数字时， 其实也是根据一笔一划识别的

如果让每个神经元负责识别数字的一部分， 最终到达识别所有数字， 那么就需要分层。

![在这里插入图片描述](https://raw.githubusercontent.com/a1254898873/images/master/202203241151035.png)

通常情况下，靠近输入的卷积层，譬如第一层卷积层，会找出一些共性的特征，如手写数字识别中第一层我们设定卷积核个数为5个，一般是找出诸如"横线"、“竖线”、“斜线”等共性特征，我们称之为basic feature，经过max pooling后，在第二层卷积层，设定卷积核个数为20个，可以找出一些相对复杂的特征，如“横折”、“左半圆”、“右半圆”等特征，越往后，卷积核设定的数目越多，越能体现label的特征就越细致，就越容易分类出来





### 6、输出

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151036.png)







## 面试题

1、Relu激活函数在零点是否可导？

在零点不可导



2、Relu激活函数的优缺点？

解决了梯度消失、爆炸的问题
计算方便，计算速度快，求导方便
加速网络训练







3、如何处理神经网络中的过拟合问题？

L1/L2正则化

dropout

数据增强







4、增大感受野的方法？

空洞卷积、池化操作、较大卷积核尺寸的卷积操作









# 数字图像处理

## 一、图像处理基础

### 1、直方图

直方图是可以对整幅图的灰度分布进行整体了解的图示，通过直方图我们可以对图像的对比度、亮度和灰度分布等有一个直观了解。

![7t7ecxq8lf](https://raw.githubusercontent.com/a1254898873/images/master/202203241151037.png)



暗图像直方图的分布都集中在灰度级的低(暗)端；
 亮图像直方图的分布集中在灰度级的高端；
 低对比度图像具有较窄的直方图，且都集中在灰度级的中部
 高对比度图像直方图的分量覆盖了很宽的灰度范围，且像素分布也相对均匀，图片的效果也相对很不错。





#### 显示直方图

```python
cv2.calcHist(images, channels, mask, histSize, ranges)
```

- 参数1：要计算的原图，以方括号的传入，如：[img]。
- 参数2：类似前面提到的dims，灰度图写[0]就行，彩色图B/G/R分别传入[0]/[1]/[2]。
- 参数3：要计算的区域ROI，计算整幅图的话，写None。
- 参数4：也叫bins,子区段数目，如果我们统计0-255每个像素值，bins=256；如果划分区间，比如0-15, 16-31…240-255这样16个区间，bins=16。
- 参数5：range,要计算的像素值范围，一般为[0,256)。



```python
hist = cv2.calcHist([img], [0], None, [256], [0, 256])  # 性能：0.025288 s
```



#### 直方图均衡化

为了提高图像处理的效果，经常会在图像处理之前进行直方图均衡化，即将图像的直方图灰度级别由集中在某一小部分灰度级分散成在所有灰度级别都有一定的覆盖，所以通过直方图均衡化的方法用来增强局部或整体的对比度。

OpenCV中用cv2.equalizeHist() 实现均衡化:

(a) 灰度图均衡，直接使用cv2.equalizeHist(gray)

(b) 彩色图均衡，分别在不同的通道均衡后合并

```python
equ = cv.equalizeHist(img)
```

CLAHE 自适应均衡化

(a) 直方图均衡化是应用于整幅图片的，会导致一些图片部位太亮，导致大部分细节丢失，因此引入自适应均衡来解决这个问题。
(b) 它在每一个小区域内（默认8×8）进行直方图均衡化。当然，如果有噪点的话，噪点会被放大，需要对小区域内的对比度进行了限制。
(c) 彩色图同样需要split为r,g,b后均衡，然后merge。

```python
# 自适应均衡化，参数可选
clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
cl1 = clahe.apply(img)
```







#### 反向投影

反向投影是一种记录给定图像中的像素点如何适应直方图模型像素分布的方式，简单来讲，反向投影就是首先计算某一特征的直方图模型，然后使用模型去寻找图像中存在的特征。反向投影在某一位置的值就是原图对应位置像素值在原图像中的总数目。

```python
import cv2
import numpy as np
from matplotlib import pyplot as plt

# roi是我们需要找到的对象或区域
roi = cv2.imread('img_roi.png')
hsv = cv2.cvtColor(roi, cv2.COLOR_BGR2HSV)

# target是我们搜索的图像
target = cv2.imread('img.jpg')
hsvt = cv2.cvtColor(target, cv2.COLOR_BGR2HSV)

# 计算对象的直方图
roihist = cv2.calcHist([hsv], [0,1], None, [180,256], [0,180,0,256])

# 标准化直方图，并应用投影
cv2.normalize(roihist, roihist, 0, 255, cv2.NORM_MINMAX)
dst = cv2.calcBackProject([hsvt], [0,1], roihist, [0,180,0,256], 1)

# 与磁盘内核进行卷积
disc = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (5,5))
cv2.filter2D(dst, -1, disc, dst)

# 阈值、二进制按位和操作
ret, thresh = cv2.threshold(dst, 50, 255, 0)
thresh = cv2.merge((thresh, thresh, thresh))
res = cv2.bitwise_and(target, thresh)

res = np.vstack((target, thresh, res))
cv2.imshow('res', res)
cv2.waitKey()
```







### 2、图像增强

##### 2.1 灰度变换

灰度变换函数描述了输入灰度值和输出灰度值之间变换关系，一旦灰度变换函数确定下来了，那么其输出的灰度值也就确定了。可见灰度变换函数的性质就决定了灰度变换所能达到的效果。用于图像灰度变换的函数主要有以下三种：

- 线性函数 （图像反转）
- 对数函数:对数和反对数变换
- 幂律函数：n次幂和n次开方变换

（1）线性变换

令r为变换前的灰度，s为变换后的灰度，则线性变换的函数：
$$
s = a \cdot r + b
$$
其中，a为直线的斜率，b为在y轴的截距。选择不同的a，b值会有不同的效果：

- a>1，增加图像的对比度
- a<1，减小图像的对比度
- 且a=1且b≠0，图像整体的灰度值上移或者下移，也就是图像整体变亮或者变暗，不会改变图像的对比度。
- 且a<0且b=0，图像的亮区域变暗，暗区域变亮
- 且a=1且b=0，恒定变换，不变
- 且a=−1且b=255，图像反转。



（2）对数变换

$$
s = c\log(1+r)
$$
其中，c是一个常数。假设r≥0,根据上图中的对数函数的曲线可以看出：对数变换，将源图像中范围较窄的低灰度值映射到范围较宽的灰度区间，同时将范围较宽的高灰度值区间映射为较窄的灰度区间，从而扩展了暗像素的值，压缩了高灰度的值，能够对图像中低灰度细节进行增强。



（3）幂律变换（伽马变换）
$$
s = cr^\gamma
$$
其中c和γ为正常数。伽马变换的效果与对数变换有点类似，当γ>1时将较窄范围的低灰度值映射为较宽范围的灰度值，同时将较宽范围的高灰度值映射为较窄范围的灰度值；当γ<1时，情况相反，与反对数变换类似。









##### 2.2 代数变换

（1）加法运算

可以用来降低图像中的随机噪声

```python
a=imread('a.jpg');
d=rgb2gray(a);
subplot(2,2,1);
imshow(d)
b=imread('b.jpeg');
e=rgb2gray(b);
subplot(2,2,2);
imshow(e)
c=imadd(d,e,'uint8');  %将两幅图像进行相加得到新的图像，注意数据类型
subplot(2,2,3);
imshow(c)
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151038.jpg)

（2）减法运算

可以用来减去背景，分割特定区域，检测场景变化

```python
  A  = imread('1.jpg');
  B  = imread('2.jpg');
  C  = imsubtract(A,B);            %将A，B两幅图像进行相减得到新的图像C
```



（3）乘法运算

通常用来进行掩模运算，获取图像中特定部分

```python
   A  = imread('1.jpg')； 
   B  = imread('2.jpg');
   C  = immultiply(A,B);            %将A，B两幅图像进行相乘得到新的图像C
```



（4）除法运算

可以用来归一化

```python
  A  = imread('1.jpg');
   B  = imread('2.jpg');
   C  = imdivide(A,B);            %将A，B两幅图像进行相除得到新的图像C
```





### 3、卷积和滤波

（1）滤波

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151039.png)

简单来说，滤波操作就是图像对应像素与掩膜（mask）的乘积之和。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151040.png)

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151041.png)



![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151042.png)

滤波后的图像大小不变





（2）卷积

卷积的原理与滤波类似。但是卷积却有着细小的差别。
卷积操作也是卷积核与图像对应位置的乘积和。但是卷积操作在做乘积之前，需要先将卷积核翻转180度，之后再做乘积。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151043.png)

卷积操作会改变图像大小

由于卷积操作会导致图像变小（损失图像边缘），所以为了保证卷积后图像大小与原图一致，经常的一种做法是人为的在卷积操作之前对图像边缘进行填充。







### 4、常见卷积核

##### 4.1 低通滤波

均值滤波和高斯滤波有如下两个共同点：

- 滤波器中元素之和为1，输出亮度与输入基本一致；
- 均为低通滤波器，主要用于图像模糊/平滑处理、消除噪点；
- 核越大，模糊程度越大；

均值滤波器从名字就可以看出，每个元素值都一样，是卷积核元素个数的倒数，这样每个输出像素就是其周围像素的均值。

```python
image = cv2.blur(image, ksize)  
```

高斯滤波器虽然元素总和也为1，但每个位置的权重不一样，权重在行和列上的分布均服从高斯分布，故称高斯滤波器。高斯分布的标准差越大，则模糊程度越大。

```python
blurred = cv2.GaussianBlur(img, (17,17), 0)
```

![低通滤波](https://raw.githubusercontent.com/a1254898873/images/master/202203241151044.png)

中值滤波器：在一连串数字｛1，4，6，8，9｝中，数字6就是这串数字的中值。由此我们可以应用到图像处理中。依然我们在图像中去3*3的矩阵，里面有9个像素点，我们将9个像素进行排序，最后将这个矩阵的中心点赋值为这九个像素的中值。

```python
lbimg=cv2.medianBlur(newimg,3)
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151045.png)



这三个滤波器主要作用均为模糊图像，或在图像预处理中消除图像中的噪点



##### 4.2 锐化卷积核

锐化卷积核属于高通滤波，从名字就可以看出，主要作用就是对图片进行锐化操作，也就是让图像的边缘更加锐利。图像的边缘往往就是变化较大的地方，也就是图像的高频部分，因此锐化卷积核就是一种高通滤波器。

该卷积核就是计算中心位置像素与周围像素的差值，差值越大则表示该元素附近的变化越大（频率越大），输出值也就越大，因此是高频滤波器的一种。锐化卷积核元素总和如果是0，则有提取图像边缘信息的效果。

![高频滤波](https://raw.githubusercontent.com/a1254898873/images/master/202203241151046.png)





##### 4.3  一阶微分算子

图像中物体的边缘往往就是变化较为剧烈的部分（高频部分），对于一个函数来说，变化越剧烈的地方，对应的导数的绝对值也就越大。图像就是一种二元函数，f ( x , y ) f(x,y)f(x,y)表示( x , y ) (x,y)(x,y)处像素的值，因此导数除了大小，还有方向。那么求图像在某方向上的一阶导数（或称图像的梯度），也就可以反映出图像在该处的变化程度，变化程度越快，在该方向的垂直方向可能就存在物体的边缘。

一阶微分算子可以计算出某个方向上物体的边缘，但往往对噪声较为敏感，且边缘检测敏感度依赖于物体的大小。

（1）Prewitt算子

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151047.png)

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151048.png)





（2）Sobel算子

Sobel算子则是Prewitt算子的改进版，对中间的元素适当进行了加权，Sobel算子之于Prewitt算子类似于高斯滤波之于均值滤波。

![一阶微分算子](https://raw.githubusercontent.com/a1254898873/images/master/202203241151049.png)



```python
	depth = cv2.CV_16S

	#求X方向梯度（创建grad_x, grad_y矩阵）
	grad_x = cv2.Sobel( src_gray, depth, 1, 0 )
	abs_grad_x = cv2.convertScaleAbs( grad_x )

	#求Y方向梯度
	grad_y = cv2.Sobel( src_gray, depth, 0, 1 )
	abs_grad_y = cv2.convertScaleAbs( grad_y )

	#合并梯度（近似）
	edgeImg = cv2.addWeighted( abs_grad_x, 0.5, abs_grad_y, 0.5, 0 )

```



（3）Scharr算子

```python
Scharrx = cv2.Scharr(o,cv2.CV_64F,1,0)
Scharry = cv2.Scharr(o,cv2.CV_64F,0,1)
Scharrx = cv2.convertScaleAbs(Scharrx) 
Scharry = cv2.convertScaleAbs(Scharry) 
Scharrxy = cv2.addWeighted(Scharrx,0.5,Scharry,0.5,0) 
```





##### 4.4  二阶微分算子

![image-20220318160844943](https://raw.githubusercontent.com/a1254898873/images/master/202203241151050.png)

Laplace算子：Laplace算子可以近似计算出图像的二阶导数，具有旋转不变性，也就是可以检测出各个方向的边缘。Laplace算子分为两种，分别考虑4-邻接（D4）和8-邻接（D8）两种邻域的二阶微分。

对于较为复杂的图像，拉普拉斯算子的效果也并不是很好，由于二阶微分一定的局限性，目前的边缘检测还不够完美

```python
dst = cv2.Laplacian(img,cv2.CV_16S,ksize=3)
dst = cv2.convertScaleAbs(dst)
```

LoG算子：Laplace算子对噪声依然很敏感。因此常常先使用高斯滤波器对图像进行平滑操作，再使用Laplace算子计算二阶微分。二者结合称为LoG算子（Laplacian of Gaussian）

![二阶微分算子](https://raw.githubusercontent.com/a1254898873/images/master/202203241151051.png)





### 5、图像金字塔

（1）高斯金字塔(Gaussianpyramid)

用来向下采样（缩小），主要的图像金字塔

```python
dst = cv2.pyrDown(src)
cv2.imshow('dst', dst)
```

（2）拉普拉斯金字塔(Laplacianpyramid)

用来从金字塔低层图像重建上层未采样图像（放大），在数字图像处理中也即是预测残差，可以对图像进行最大程度的还原，配合高斯金字塔一起使用。

```python
src1 = cv2.pyrUp(dst)
cv2.imshow('src1', src1)
```







## 二、opencv基本操作

### 1、导入

#### （1）图片导入

```python
import cv2
import numpy as np
from matplotlib import pyplot as plt

img = cv2.imread('watch.jpg',cv2.IMREAD_GRAYSCALE)
cv2.imshow('image',img)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

首先，我们正在导入一些东西，我已经安装了这三个模块。接下来，我们将img定义为cv2.read(image file, parms)。默认值是IMREAD_COLOR，这是没有任何 alpha 通道的颜色。如果你不熟悉，alpha 是不透明度（与透明度相反）。如果你需要保留 Alpha 通道，也可以使用IMREAD_UNCHANGED。很多时候，你会读取颜色版本，然后将其转换为灰度。



#### （2）加载视频

```python
import numpy as np
import cv2

cap = cv2.VideoCapture(0)
# cap = cv2.VideoCapture('vtest.avi')

while(True):
    ret, frame = cap.read()
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)

    cv2.imshow('frame',gray)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

cap.release()
cv2.destroyAllWindows()
```

这段代码启动了一个无限循环（稍后将被break语句打破），其中ret和frame被定义为cap.read()。 基本上，ret是一个代表是否有返回的布尔值，frame是每个返回的帧。 如果没有帧，你不会得到错误，你会得到None。







### 2、在图像上绘制和写字



```python
cv2.rectangle(img,(15,25),(200,150),(0,0,255),15)
```

这里的参数是图像，左上角坐标，右下角坐标，颜色和线条粗细。



```python
cv2.rectangle(img,(500,250),(1000,500),(0,0,255),15)
```

参数表示依次为： （图片，长方形框左上角坐标, 长方形框右下角坐标， 字体颜色，字体粗细）



```python
imgzi = cv2.putText(img, '000', (50, 50), font, 1.2, (255, 255, 255), 2)
```

各参数依次是：图片，添加的文字，左上角坐标，字体，字体大小，颜色，字体粗细





### 3、图像算术和逻辑运算



```python
add = cv2.add(img1,img2)
```

例如：(155,211,79) + (50, 170, 200) = 205, 381, 279...转换为(205, 255,255)



我们可以添加图像，并可以假设每个图像都有不同的“权重”。 

```python
import cv2
import numpy as np

img1 = cv2.imread('3D-Matplotlib.png')
img2 = cv2.imread('mainsvmimage.png')

weighted = cv2.addWeighted(img1, 0.6, img2, 0.4, 0)
cv2.imshow('weighted',weighted)
cv2.waitKey(0)
cv2.destroyAllWindows()
```



### 4、阈值

阈值的思想是进一步简化视觉数据的分析。首先，你可以转换为灰度，但是你必须考虑灰度仍然有至少 255 个值。阈值可以做的事情，在最基本的层面上，是基于阈值将所有东西都转换成白色或黑色。比方说，我们希望阈值为 125（最大为 255），那么 125 以下的所有内容都将被转换为 0 或黑色，而高于 125 的所有内容都将被转换为 255 或白色。如果你像平常一样转换成灰度，你会变成白色和黑色。如果你不转换灰度，你会得到二值化的图片，但会有颜色。

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151052.png)

```python
retval, threshold = cv2.threshold(img, 10, 255, cv2.THRESH_BINARY)
```

二元阈值是个简单的“是或不是”的阈值，其中像素为 255 或 0。在很多情况下，这是白色或黑色，但我们已经为我们的图像保留了颜色，所以它仍然是彩色的。 这里的第一个参数是图像。 下一个参数是阈值，我们选择 10。下一个是最大值，我们选择为 255。最后是阈值类型，我们选择了THRESH_BINARY。 通常情况下，10 的阈值会有点差。 我们选择 10，因为这是低光照的图片，所以我们选择低的数字。 通常 125-150 左右的东西可能效果最好。

```python
import cv2
import numpy as np

grayscaled = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)
retval, threshold = cv2.threshold(grayscaled, 10, 255, cv2.THRESH_BINARY)
cv2.imshow('original',img)
cv2.imshow('threshold',threshold)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151053.png)

```python
import cv2
import numpy as np

th = cv2.adaptiveThreshold(grayscaled, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 115, 1)
cv2.imshow('original',img)
cv2.imshow('Adaptive threshold',th)
cv2.waitKey(0)
cv2.destroyAllWindows()
```

![img](https://raw.githubusercontent.com/a1254898873/images/master/202203241151054.png)





### 5、颜色过滤

HSV(色调，饱和度，值)
就像我们用来表示RGB和BGR等图像的最常见颜色空间一样，HSV也是一个颜色空间，其中H表示色调，S表示饱和度，V表示值。因为我们已经知道什么是色调，让我们看看饱和度和值。

饱和度
饱和度使颜色变得纯净。纯色中没有灰色。颜色中混合的灰色越多，饱和度越低。饱和度值通常从0到100%测量，但在OpenCV中，饱和度的范围是从0到255。

值
这是一种颜色亮度的量度。当亮度值最大时，颜色变为白色，当亮度值最小时，颜色变为黑色。这通常是0到100%，但在OpenCV中，值的范围是从0到255。

```python

import cv2 as cv
# 读取图片
rgb_img = cv.imread("assets/tenis1.jpg", cv.IMREAD_COLOR)
cv.imshow("rgb_img",rgb_img)
# 将BGR颜色空间转成HSV空间
hsv_img = cv.cvtColor(rgb_img, cv.COLOR_BGR2HSV)

# 定义范围 网球颜色范围
lower_color = (30,120,130)
upper_color = (60,255,255)

# 查找颜色
mask_img = cv.inRange(hsv_img, lower_color, upper_color)
# 在颜色范围内的内容是白色, 其它为黑色
cv.imshow("mask_img",mask_img)

cv.waitKey(0)
cv.destroyAllWindows()
```





### 6、边缘检测

OpenCV为我们提供了一个非常方便的边缘检测接口Canny函数，该函数的命名是以其发明者Jhon F.Canny命名的，自1986年被发明后该算法就一直很受欢迎，其不容易受噪声的干扰，它的双阈值法可以分别检测到强边缘和弱边缘，并且仅当弱边缘与强边缘相连时，才将弱边缘包含在输出结果中，这就保障了检测到真正的弱边缘。

```python
Canny(image, threshold1, threshold2, edges=None, apertureSize=None, L2gradient=None)
```

阈值越小能检测到的边界就越多
